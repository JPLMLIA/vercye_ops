{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"VeRCYe Documentation","text":"<p>This documentation aims to provides a comprehensive guide to running the Versatile Crop Yield Estimate (VeRCYe) pipeline. The original VeRCYe algorithm is published here.</p> <p>Currently the documentation is a work in progress, so please visit again soon.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Tools to greatly reduce manual effort required for executing the VERYCE crop yield estimate pipeline.</li> <li>All workflow steps are wrapped in a well-documented CLI interface to permit step by step execution.</li> <li>The core CLI steps are also wrapped in a Snakemake-based data processing pipeline to batch execute yield estimates in an easy to run and reproducible manner.</li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>The VeRCYe pipeline is split into two main components:</p> <ul> <li>LAI Generation: Download remotely sensed imagery and predict Leaf Area Index (LAI) values per pixel.</li> <li>Yield Simulation and Prediction: Simulate numerous likely configurations using APSIM and identify the best-matching simulations with the LAI data. This step also includes evaluation and reporting tools.</li> </ul>"},{"location":"#setup","title":"Setup","text":""},{"location":"#0-clone-this-repository","title":"0. Clone this repository","text":"<pre><code>git clone https://github.com/JPLMLIA/vercye_ops.git\n</code></pre>"},{"location":"#1-check-python-version-and-gdal","title":"1. Check Python Version and GDAL","text":"<p>This repository has been tested and run with <code>python 3.10.16</code> and with <code>gdal==3.1.0</code>. Ensure you have installed the corresponding versions (<code>python --version</code> and <code>gdalinfo --version</code>). If you are running your code on a shared cluster, you might have to run <code>module load gdal/3.1.0</code>, before being able to use <code>GDAL</code>.</p>"},{"location":"#2-install-the-requirements","title":"2. Install the requirements","text":"<p>Navigate to this package's root directory and run:</p> <pre><code>conda install --yes --file requirements.txt\n# or\npip install -r requirements.txt\n</code></pre> <p>Note: As of June 2024, if using <code>conda</code>, you may also need to install <code>Snakemake</code> and a specific dependency manually via pip:</p> <p><code>bash pip install snakemake pulp==2.7.0</code></p>"},{"location":"#3-install-the-vercye-package","title":"3. Install the VeRCYe package","text":"<p>From the root directory, run:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"#4-install-apsimx","title":"4. Install APSIMX","text":"<p>There are two options for running APSIM:</p> <ul> <li>Using Docker: Simply set a parameter during configuration of your yield study. The Docker container will build automatically. (Ensure <code>docker</code> is installed.)</li> <li>Building the binary manually: See instructions in vercye_ops/apsim/README.md.</li> </ul> <p>Note: If running on UMD systems, APSIM is pre-installed at:</p> <p><code>/gpfs/data1/cmongp2/wronk/Builds/ApsimX/bin/Release/net6.0/Models</code></p>"},{"location":"#5-install-jq","title":"5. Install <code>jq</code>","text":"<p>To manipulate JSON files via the command line:</p> <ul> <li>Install from https://jqlang.github.io/jq/</li> <li>Or on HPC systems, load it with:</li> </ul> <pre><code>module load jq\n</code></pre>"},{"location":"#running-your-first-yield-study","title":"Running your first yield study","text":"<p>You will first have to generate LAI data from remotely sensed imagery. Refer to the LAI Creation Guide for details.</p> <p>Once you have generated the LAI data, you can run your yield study, by following the Running a Yieldstudy Guide.</p>"},{"location":"#technical-details","title":"Technical Details","text":"<p>The technical implementation details are outlined in he Architecture Section. Fore more details check out the code in <code>vercye_ops</code>.</p>"},{"location":"#development","title":"Development","text":"<p>Development tipps and best practices are documented under Development Tipps</p>"},{"location":"LAI/intro/","title":"VeRCYe LAI Generation","text":"<p>VeRCYE is designed to identify best matching APSIM simulations with actual remotely sensed Leaf Area Index (LAI) data. This documentation covers the LAI data generation pipeline, which is a separate but essential component of the VeRCYE workflow. The LAI is not a true remotely sensed value, but is rather estimated from a  combination of bands using neural networks that were converted to Pytorch from the Leaf Toolbox.</p> <p>The LAI creation pipeline described in this documentation transforms Sentinel-2 satellite imagery into LAI estimates that can be used by the VeRCYE algorithm. Currently only Sentinel-2 data is supported, but a version using Harmonized Landsat-Sentinel Imagery is planned.</p> <p>We've intentionally decoupled the LAI generation from the standard VeRCYE pipeline for several practical reasons: 1. It requires downloading and processing substantial amounts of satellite data 2. Once generated, LAI data for a region can be reused with different VeRCYE configurations</p>"},{"location":"LAI/intro/#lai-generation","title":"LAI Generation","text":"<p>We provide two methods for exporting remotely sensed imagery and deriving LAI products:</p> <p>A: Exporting RS imagery from Google Earth Engine B: Downloading RS imagery through an open source STAC catalog and data hosted on AWS.</p>"},{"location":"LAI/intro/#a-google-earth-engine-exported","title":"A: Google Earth Engine Exported","text":"<p>This approach lets Google Earth Engine handle the majority of data preprocessing and exports daily Sentinel-2 Mosaics from Google Earth Engine to either a Google Drive or a Google Cloud Storage Bucket, from which it can be downloaded to the HPC environment for deriving the LAI product. The data in Google Earth Engine is the <code>S2_SR_Harmonized</code> collection, in which all data is harmonized to be in the same range, even if different processing baselines are used.</p> <p>Pro: - Directly Export Mosaics with bands resampled to the same resolution and CRS - Strong Cloud Masking Algorithm (Google Cloud Score Plus)</p> <p>Con - Slow for large regions, due to limited number of parallel export processes - Exported data is exported to either Google Drive (Free) or Google Cloud Storage (Fees apply), and downloaded from there, but requires more manual setup which might be tedious especially on remote systems.</p>"},{"location":"LAI/intro/#b-stac-aws-export","title":"B: STAC &amp; AWS Export","text":"<p>This approach queries a STAC catalog hosted by Element84 on AWS to identify all Sentinel-2 Tiles intersecting the region of interest within the timespan. The individual tiles are then downloaded from an AWS bucket. This data uses the <code>Sentinel-2 L2A Colection 1</code> data in which all historial data was processed using <code>Processing Baseline 5.0</code>.</p> <p>Pro: - Very Fast Download in HPC environment due to high level of parallelism - Completely free download of data - Harmonized to data in <code>Sentinel-2 L2A Colection 1</code> - all data processed using modern Baseline 5.0.</p> <p>Con: - Less Accurate Cloudmask in comparison to Google Cloud Score Plus. Cloud mask is based on SCL + S2-Cloudless. - As of May 27th 2025, <code>Sentinel-2 L2A Colection 1</code> does not contain data for 2022 and parts of 2023. According to ESA this backfill is scheduled to be completed until Q2 2025.</p> <p>Instructions on how to run each LAI creation pieline are detailed in LAI Generation Instructions.</p>"},{"location":"LAI/running/","title":"\ud83c\udf3f LAI Generation","text":"<p>The pipeline produces LAI products for VERCYe and is intended for scaled deployment on servers or HPC with minimal human intervention. The Sentinel-2 LAI model is by Fernandes et al. from https://github.com/rfernand387/LEAF-Toolbox. We provide two methods for exporting remotely sensed imagery and deriving LAI products:</p> <p>A: Exporting RS imagery from Google Earth Engine (slow, more setup required, better cloudmasks) B: Downloading RS imagery through an open source STAC catalog and data hosted on AWS (fast, inferior cloudmasking).</p> <p>The individual advantages are detailed in the introduction. This document details the instruction on how to download remotely sensed imagery and derive LAI data. For both approaches we provide pipelines that simply require specifying a configuration and then handle the complete process from exporting and downloading remotely sensed imagery to cloudmasking and deriving LAI estimates. Details of the invididual components of the pipelines can be found in the readme of the corresponding folders.</p>"},{"location":"LAI/running/#prequisites","title":"Prequisites","text":"<p>Install the requirements as detailed in Introduction - Setup</p>"},{"location":"LAI/running/#a-google-earth-engine-pipeline","title":"A - Google Earth Engine Pipeline","text":"<p>Step 1: GeoJSON Extraction Extract geosjsons from a shapefile for each region of interest. Typically, you will want to break down large areas into individual geometries, such as from the <code>Admin 1 or 2</code> level depending on their size in the country of interest.</p> <p>While it is possible, to also provide a national scale geometry directly, in the past we noticed the export through GEE to be significantly slower then when processing multiple e.g <code>admin2</code> geometries in parallel. We therefore do not reccomend providing a single national geometry.</p> <p>Use the <code>vercye_ops/lai/0_build_library.py</code> helper script to extract individual GeoJSONS from a shapefile. Ensure that the shapefile only contains geometries at the same administrative level (e.g do NOT mix polygons for states and districts in the same shapefile).</p> <pre><code>python 0_build_library.py --help\nUsage: 0_build_library.py [OPTIONS] SHP_FPATH\n\n  Wrapper around geojson generation func\n\nOptions:\n  --admin_name_col [str]  Column name in the shapefile\\'s attribute table\n                          containg the geometries administrative name (e.g NAME_1).\n  --output_head_dir DIRECTORY   Head directory where the region output dirs\n                                will be created.\n  --verbose                     Print verbose output.\n  --help                        Show this message and exit.\n</code></pre> <p>Step2: Create your Google Earth Engine Credentials Follow the Google Drive Python Quickstart to download a <code>client_secret.json</code>: https://developers.google.com/drive/api/quickstart/python</p> <p>[!NOTE] Google OAuth requires accessing a server-side browser via X11 forwarding to produce a <code>token.json</code>. This can get complicated, involving Xming or Xquartz along with the appropriate <code>$DISPLAY</code> and <code>.ssh/config</code> parameters. It may be easier to just run this locally to produce the <code>token.json</code>, then transfer the token to the server. For this, you will have to run <code>vercye_ops/vercye_ops/lai/lai_creation_GEE/1_1_gee_export_S2.py</code> with <code>--export_mode drive</code> and <code>--gdrive_credentials /path.to/your/credentials.json</code> and some other dummy parameters. You can cancel the run, once you see that the earth engine login is completed. This will then produce the token that you have to transfer to the server. Otherwise, please discuss with your system administrator.</p> <p>Step 3: Setup the GEE-LAI Pipeline Configuration Create a LAI config file that defines the parameters of your study.</p> <p>3.1 Copy <code>vercye_ops/lai/lai_creation_GEE/example_config.yaml</code> to <code>vercye_ops/lai/lai_creation_GEE/custom_configs/your_config_name.yaml</code></p> <p>3.2 Set the parameters in <code>vercye_ops/lai/lai_creation_GEE/custom_configs/your_config_name.yaml</code>:</p> <pre><code># Folder where GeoJSONS from Step1 are stored (output_head_dir from 0_build_library.py)\ngeojsons_dir: 'lai/regions/'\n\n# Your Earth Engine project ID\nee_project: 'ee-project-id'\n\n# Output directory for the LAI data and intermediate files\noutput_base_dir: 'outputdir'\n\n# Path to the Google Earth Engine service account credentials\ngdrive_credentials_path: '/path/to/client_secret.json'\n\n# Timepoints for which to create LAI\ntimepoints:\n  2023:\n      start_date: '2023-10-10'\n      end_date: '2024-04-03'\n  2024:\n    start_date: '2024-10-10'\n    end_date: '2025-04-03'\n\n# Spatial resolution in meters. Should typically be 10 or 20.\nresolution: 20\n\n# Set to True to merge the LAI produced for each region into single regions daily VRT files\nmerge_regions_lai: True\ncombined_region_name: 'merged_regions'\n</code></pre> <p>[!NOTE] If you only have very few regions and timepoints it might make sense to split a timepoint into multiple timepoints. E.g instead of having a timepoint with <code>start_date: '2023-10-10', end_date: '2024-04-03'</code> you would create multiple timepoints such as <code>start_date: '2023-10-10', end_date: '2023-12-01'</code>, <code>start_date: '2023-12-01', end_date: '2024-02-01'</code>, and <code>start_date: '2024-02-01', end_date: '2023-04-03'</code>. This allows to leverage more parallel processing capabilities, since you are able to submit about 10 jobs in parallel.</p> <p>Step 4: Navigate to the Pipeline</p> <pre><code>cd vercye_ops/lai/lai_creation_GEE\n</code></pre> <p>Step 5: Run the Pipeline</p> <pre><code>snakemake --configfile /your/configfile.yaml --cores 10\n</code></pre> <p>Replace <code>/your/configfile.yaml</code> with the actual path to your configuration file from Step 2.</p> <p>What Happens In The Pipeline? The pipeline orchestrates a sophisticated workflow:</p> <ul> <li>Export Management: Submits up to 10 parallel jobs to GEE for Sentinel-2 mosaic exports from your regions and date ranges</li> <li>Smart Downloads: Automatically downloads exported data from Google Drive to your local machine</li> <li>Storage Cleanup: Frees up Google Drive storage immediately after download</li> <li>Data Standardization: Processes and standardizes the imagery data if it was split into multiple files from GEE</li> <li>LAI Generation: Creates LAI products for each processed file</li> <li>Optional Merging: Combines regional data into single daily files if specified in your config</li> </ul> <p>Performance Tuning The --cores parameter controls parallel processing. While you can adjust this based on your system resources, there's usually no benefit to going beyond 10 cores - that's the maximum number of simultaneous export jobs allowed under GEE's educational and non-profit licenses.</p> <p>Output Structure Your LAI products will land in different locations depending on your configuration:</p> <ul> <li>Merged regional data: output_base_dir/merged_regions_lai (single daily files covering all regions)</li> <li>Individual regional data: output_base_dir/lai (separate files per region)</li> </ul>"},{"location":"LAI/running/#b-stac-catalog-aws-pipeline","title":"B - STAC Catalog &amp; AWS Pipeline","text":"<p>The STAC pipeline fetches Sentinel-2 Imagery from an AWS bucket hosted by Element84. It uses data from <code>Sentinel-2 L2A Collection 1</code>. All this data has been processed with <code>Baseline 5.0</code>.</p> <p>To generate daily LAI data for your region of interest follow the steps blow:</p> <p>Step 1: Prepare Your Area of Interest Prepare a GeoJSON file representing the convex hull of your region.</p> <p>In QGIS, this can be done by:</p> <ol> <li><code>Vector \u2192 Geoprocessing Tools \u2192 Dissolve</code></li> <li>Then: <code>Vector \u2192 Geoprocessing Tools \u2192 Convex Hull</code></li> <li>Export the resulting layer as GeoJSON</li> </ol> <p>Step 2: Define Your Configuration</p> <p>Here's an example of how you'd process a multiple years of Morocco data at 20m resolution (Save as <code>config.yaml</code>):</p> <pre><code>date_ranges:\n  - start_date: \"2019-04-01\"\n    end_date: \"2019-06-30\"\n  - start_date: \"2020-03-15\"\n    end_date: \"2020-07-15\"\n  - start_date: \"2021-05-01\"\n    end_date: \"2021-09-30\"\n\nresolution: 20\ngeojson_path: /data/morocco.geojson\nout_dir: /data/morocco/lai\nregion_out_prefix: morocco\nfrom_step: 0\nnum_cores: 64\nchunk_days: 30\n</code></pre> <ul> <li> <p><code>date_ranges</code>: Define multiple seasonal or arbitrary time windows to process (in YYY-MM-DD format).</p> </li> <li> <p><code>resolution</code>: Spatial resolution in meters. (Typically 10 or 20)</p> </li> <li><code>geojson-path</code>: Path to your convex hull geojson.</li> <li><code>out_dir</code>: Output directory for all generated data.</li> <li><code>region_out_prefix</code>: Prefix for the output VRT filenames - typically the name of the GeoJSON region.</li> <li><code>from_step</code>: Controls which part of the pipeline to resume from (0\u20133). Should be at 0 if not trying to recover a crashed run.</li> <li><code>chunk_days</code>: Number of days to process in each batch. Default is 30 days. Can be used to control storage usage by avoiding to keep more than chunk-days of original tile data on disk at once.</li> <li><code>num_cores</code>: Number of cores to use. Default is 1 (sequential). Increase for faster processing on multi-core systems.</li> </ul> <p>Step 3: Navigate to the Pipeline</p> <pre><code>cd vercye_ops/lai/lai_creation_STAC\n</code></pre> <p>Step 3: Run the LAI Generation Pipeline</p> <pre><code>python run_stac_dl_pipeline.py /path/to/your/config.yaml\n</code></pre> <p>Pipeline Steps Breakdown - Step 0: Download imagery from AWS - Step 1: Generate LAI for individual tiles - Step 2: Clean up temporary files - Step 3: Build final VRT mosaics</p> <p>After the pipeline finishes , you'll find a <code>merged-lai</code> directory in your <code>out_dir</code> packed with daily .vrt files. Each file contains LAI data for your entire region, covering all tiles that had usable imagery on that date.</p> <p>Happy LAI generating! \ud83d\udef0\ufe0f\ud83c\udf31</p>"},{"location":"Vercye/architecture/","title":"\ud83d\udcd8 Vercye Architecture Documentation","text":""},{"location":"Vercye/architecture/#overview","title":"\ud83d\udd0d Overview","text":"<p>The Vercye system orchestrates a modular, region-based crop yield simulation pipeline using Snakemake for dependency resolution and scalability. Each processing step is encapsulated in a standalone script, executed conditionally and in the correct order.</p>"},{"location":"Vercye/architecture/#scripts","title":"\ud83d\udcc2 Scripts","text":"<p>Each script is self-documented via CLI help using the <code>--help</code> flag. For additional specifics, you can run the script directly in the terminal.</p>"},{"location":"Vercye/architecture/#pipeline-execution-with-snakemake","title":"\u2699\ufe0f Pipeline Execution with Snakemake","text":"<p>Snakemake defines workflows as rules. Each rule specifies expected inputs and outputs, enabling:</p> <ul> <li>Automatic dependency resolution \u2013 Only missing or outdated outputs are recomputed.</li> <li>Resumability \u2013 Failed executions can resume from the point of failure.</li> <li>Parallelization \u2013 Regions and timepoints are processed independently, increasing efficiency.</li> </ul>"},{"location":"Vercye/architecture/#config-file","title":"\ud83e\uddfe Config File","text":"<p>Every pipeline run uses a configuration file (YAML), which defines key parameters like:</p> <ul> <li>Data sources (e.g., LAI, meteorology)</li> <li>Resolution and CRS specs</li> <li>Simulation ranges (years, timepoints)</li> <li>Runtime limits and job parallelization caps</li> <li>Regions</li> </ul> <p>This design ensures reproducibility and clean separation of configuration from logic.</p>"},{"location":"Vercye/architecture/#pipeline-logic","title":"\ud83d\udd04 Pipeline Logic","text":"<p>The following lists a high level overview of some of the most important features of the snakemake based pipeline steps (non-exhaustive). The complete logic is defined in <code>vecrye_ops/snakemake/Snakefile</code>.</p>"},{"location":"Vercye/architecture/#1-cropmask-reprojection","title":"1. Cropmask Reprojection","text":"<ul> <li>Rule: <code>reproject_cropmask</code></li> <li>Aligns the cropmask to match CRS and resolution of LAI data.</li> <li>Requirement: All LAI files in a year must have the same resolution and CRS.</li> </ul>"},{"location":"Vercye/architecture/#2-region-masking","title":"2. Region Masking","text":"<ul> <li>Rule: <code>constrain_cropmask</code></li> <li>Crops the reprojected cropmask to each region for spatial filtering.</li> </ul>"},{"location":"Vercye/architecture/#3-region-validation","title":"3. Region Validation","text":"<ul> <li>Rule: <code>validate_region_has_cropland</code></li> <li>Skips regions with too few cropland pixels to avoid wasting compute.</li> <li>Followed by a checkpoint: <code>all_regions_validated</code>, ensuring only valid regions are processed further. This enforces snakemake to first execute all validation jobs before continuing with processing the valid regions only.</li> </ul>"},{"location":"Vercye/architecture/#4-meteorological-data-acquisition","title":"4. Meteorological Data Acquisition","text":"<p>Supported Sources: - ERA5 (via Google Earth Engine): Max 10 concurrent jobs. - NASAPower: Uses a global cache to avoid API rate limits.     - First job: One-time cache fill per region for its full date range (single job per region to avoid race conditions in cache write).     - Followed by region-timepoint-specific fetches. - CHIRPS: Requires global files to be locally available.</p>"},{"location":"Vercye/architecture/#5-lai-analysis","title":"5. LAI Analysis","text":"<ul> <li>Rule: <code>lai_analysis</code></li> <li>Computes daily LAI stats for the cropland pixels per region (e.g., mean LAI).</li> <li>Optimized with windowed reading for performance.</li> </ul>"},{"location":"Vercye/architecture/#6-apsim-simulation","title":"6. APSIM Simulation","text":"<ul> <li>Rule: <code>run_apsim</code></li> <li>Simulates many parameter combinations to model yield/LAI curves through APSIM.</li> <li>Supports both Dockerized and local APSIM installations depending on config parameter.</li> </ul>"},{"location":"Vercye/architecture/#7-simulation-matching","title":"7. Simulation Matching","text":"<ul> <li>Rule: <code>match_sim_real</code></li> <li>Compares simulated outputs with observed LAI stats.</li> <li>Selects best-matching simulations based on curve similarity.</li> </ul>"},{"location":"Vercye/architecture/#8-yield-map-generation","title":"8. Yield Map Generation","text":"<ul> <li>Rule: <code>generate_converted_lai_map</code></li> <li>Uses conversion factors to transform predicted yield into pixel-level raster maps.</li> </ul>"},{"location":"Vercye/architecture/#9-yield-estimation-aggregation","title":"9. Yield Estimation &amp; Aggregation","text":"<ul> <li>Rule: <code>estimate_total_yield</code></li> <li>Aggregates pixel-level predictions into total yield per region.</li> <li>Additional aggregate rules summarize results across all regions and timepoints.</li> </ul>"},{"location":"Vercye/architecture/#10-evaluation","title":"10. Evaluation","text":"<ul> <li>Rule: <code>evaluate_yield_estimates</code></li> <li>Compares predictions to ground truth using metrics like:</li> <li>Mean Absolute Error (MAE)</li> <li>Root Mean Squared Error (RMSE)</li> <li>R\u00b2 (coefficient of determination)</li> <li>Relative RMSE</li> </ul>"},{"location":"Vercye/architecture/#11-final-outputs","title":"11. Final Outputs","text":""},{"location":"Vercye/architecture/#per-timepoint","title":"Per-Timepoint:","text":"<ul> <li>PDF report (via <code>generate_final_report</code>)</li> <li>Aggregated CSVs and maps</li> </ul>"},{"location":"Vercye/architecture/#across-years","title":"Across Years:","text":"<ul> <li>Rule: <code>generate_multiyear_comparison</code></li> <li>Creates multi-year overviews and trend visualizations</li> </ul>"},{"location":"Vercye/architecture/#understanding-the-snakefile","title":"Understanding the Snakefile","text":"<p>In some parts you might find the <code>Snakefile</code> hard to interpret directly, which is mainly due to two factors:</p> <ul> <li> <p>Conditional Logic: Such as only running the evaluation rule if reference data files are found in the directories. This is implemented a bit hacky through a the <code>get_evaluation_results_path_func</code> that checks which timepoints have reference data, and creating a dependancy for the final rule (<code>final_report</code>).</p> </li> <li> <p>Checkpointing: This feature from <code>Snakemake</code> allows us, to not run all downstream jobs on every region. By first validating which region has sufficient cropland to be considered for analysis, the pipeline is split into a two-step approach. However, this comes at the cost of making the Snakefile slightly harder to follow: For ensuring dependancies only on valid regions instead of all regions in the snakefile, downstream jobs use <code>get_valid_regions</code> helper functions, which requires the checkpoint to have passed. Additionally, if other outputs should be produced, this becomes slightly hacky due to the way <code>Snakemake</code> handles things (see the <code>sim_match_report_workaround</code> input in the rule <code>all</code> as an example for how to deal with such cases).</p> </li> </ul>"},{"location":"Vercye/architecture/#pipeline-diagram","title":"\ud83d\udcce Pipeline Diagram","text":"<p>See the pipeline diagram <code>vercye_pipeline_highlevel.png</code> for a visual reference of the individual rules and their dependancies / outputs.</p>"},{"location":"Vercye/devtipps/","title":"Devtipps","text":""},{"location":"Vercye/devtipps/#snakemake-niceties","title":"Snakemake Niceties","text":"<p>To visualize DAGs of the pipeline or generate a report, use the following commands:</p> <pre><code># Visualizing the processing pipeline for a single execution\nsnakemake --configfile &lt;your_config.yaml&gt; --rulegraph | dot -Tpdf &gt; dag_rulegraph_template.pdf\n\n# Visualizing the graph for all pipeline executions\nsnakemake --configfile &lt;your_config.yaml&gt; --dag &gt; dag.dot\ndot -Tpdf dag.dot &gt; dag_custom.pdf\n\n# Generate a report for a finished pipeline execution (to set graph and runtimes)\nsnakemake --configfile &lt;your_config.yaml&gt; --report report.html\n</code></pre> <p>Other helpful command line args are documented on the snakemake CLI page.</p>"},{"location":"Vercye/devtipps/#tests","title":"Tests","text":"<p>Tests are handled by pytest. Navigate to the root directory and execute <code>pytest</code> to run them.</p>"},{"location":"Vercye/inputs/","title":"VeRCYe Pipeline Inputs Documentation","text":""},{"location":"Vercye/inputs/#overview","title":"Overview","text":"<p>The VeRCYe pipeline enables large-scale yield studies by organizing simulation data within a structured directory and processing it with <code>snakemake</code>. This document details the required input files, directory structure, and configurable parameters. </p>"},{"location":"Vercye/inputs/#1-yield-study-setup","title":"1. Yield Study Setup","text":"<p>To run a yieldstudy, you will need two things:</p> <ol> <li>Base Directory: A directory that includes all your regions of interest, APSIM configurations and Reference Data.</li> <li>Configuration: A configuration file for VeRCYe, specifying different parameters for the study.</li> </ol> <p>We provide an example setup under Example Setup.</p> <p>In this document we detail the steps for the input setup. However, we recommend using the <code>setup_helper.ipynb</code> as outlined in Running VeRCYe.</p>"},{"location":"Vercye/inputs/#simulation-head-directory","title":"Simulation Head Directory","text":"<p>Your yield study is structured within a single directory, referred to as the simulation head directory. This directory must follow a predefined structure to ensure compatibility with the pipeline.</p>"},{"location":"Vercye/inputs/#directory-structure","title":"Directory Structure","text":"<pre><code>head_dir/\n|   snakemake_config.yaml\n|---Year1/\n|   |   groundtruth-primary-Year1.csv (optional)\n|   |---TimePoint-1/\n|   |   |---region1/\n|   |   |   |   region1.geojson\n|   |   |   |   region1_template.apsimx\n|   |---TimePoint-N/\n|       |---regionN/\n|           |   regionN.geojson\n|           |   regionN_template.apsimx\n|---Year2/\n    ...\n</code></pre> <p>Each year contains timepoints, and each timepoint contains regions with their respective <code>geojson</code> and <code>apsimx</code> template files. The names in this structure are just descriptive placeholders and should be adjusted as described below.</p>"},{"location":"Vercye/inputs/#2-region-geojson-files","title":"2. Region GeoJSON Files","text":""},{"location":"Vercye/inputs/#purpose","title":"Purpose","text":"<p>Each Region of Interest (ROI) is represented as a GeoJSON file within its respective timepoint directory.</p>"},{"location":"Vercye/inputs/#converting-shapefiles-to-geojson","title":"Converting Shapefiles to GeoJSON","text":"<p>We expect your data to initially be a shapefile (.shp). To extract individual regions into GeoJSONs, we provide a conversion script. If your shapefile has</p> <ul> <li>A: Mixed Administrative Levels: Use <code>apsim/prepare_shapefile.py</code> to standardize the shapefile before conversion. Then proceed with B.</li> <li>B: Single Administrative Level: Use <code>apsim/convert_shapefile_to_geojson.py</code> if the shapefile has a uniform administrative level.</li> </ul> <p>[!Warning] Ensure your shapefiles contains only geometries at the same administrative level if skipping <code>prepare_shapefile.py</code>!</p>"},{"location":"Vercye/inputs/#file-naming-convention","title":"File Naming Convention","text":"<p>Each GeoJSON file must follow the format:</p> <pre><code>regionname.geojson\n</code></pre> <p>For example, if studying California, the file structure would be:</p> <pre><code>2024/T-0/california/california.geojson\n</code></pre> <p>If you do not use our conversion from shapefile to GeoJSON, you will need to manually ensure that each GeoJSON contains a centroid column that has the same format as extracted in <code>convert_shapefile_to_geojson.py</code>.</p>"},{"location":"Vercye/inputs/#3-years-and-timepoints","title":"3. Years and Timepoints","text":""},{"location":"Vercye/inputs/#defining-years","title":"Defining Years","text":"<p>Years should be named numerically (e.g., <code>2024</code>, <code>2025</code>) to represent different simulation periods.</p>"},{"location":"Vercye/inputs/#defining-timepoints","title":"Defining Timepoints","text":"<p>Each timepoint represents a simulation scenario, e.g., using all available meteorological data vs. limiting it to 30 days before the latest observation.</p> <p>Each timepoint must define:</p> <ul> <li>APSIM Simulation Start &amp; End Dates</li> <li>Meteorological Data Start &amp; End Dates</li> <li>LAI (Leaf Area Index) Data Start &amp; End Dates</li> </ul> <p>Years and Timepoints are referenced in <code>snakemake_config.yaml</code> using their respective names. Therefore the folder names must match these.</p>"},{"location":"Vercye/inputs/#4-apsimx-templates","title":"4. APSIMX Templates","text":""},{"location":"Vercye/inputs/#purpose_1","title":"Purpose","text":"<p>Each region and timepoint requires an APSIMX template (<code>regionname_template.apsimx</code>). This file defines crop and soil and other parameters for APSIM. The dates (<code>Models.Clock</code>) in the APSIM file must align with the simulation dates set in <code>snakemake_config.yaml</code>! This is often the only change that needs to be applied for using the same APSIM files for different years.</p> <p>If you are not using the <code>setup_helper.ipynb</code> you will likely want to copy the same file to a number of directories (regions):</p> <pre><code>cd path/to/apsim_simulation_20240607/2021/T-0  # Path where regions exist (e.g., from above shapefile conversion)\nsource_file=\"/path/to/my/template.apsimx\"; for dir in *; do cp \"$source_file\" \"${dir}/${dir}_template.apsimx\"; done\n</code></pre> <p>Adjustments for e.g soil properties and simulation constraints must be manually configured with domain knowledge.</p>"},{"location":"Vercye/inputs/#5-validation-data-optional","title":"5. Validation Data (Optional)","text":"<p>If ground-truth yield data is available, it should be included as <code>groundtruth_{AggregationLevel}-{Year}.csv</code> in the corresponding year directory. Hereby, <code>aggregation level</code>, specifies how the simulation level regions should be aggregated and must be aliged with the <code>eval_params.aggregation_levels</code> in your <code>snakemake_config.yaml</code>. The <code>year</code> must match the corresponding year directory.</p>"},{"location":"Vercye/inputs/#reference-csv-specification","title":"Reference CSV Specification","text":"Column Name Description <code>region</code> Name matching GeoJSON folder (for <code>primary aggregation level</code>) or matching attribute table column values for custom aggregation level <code>reported_mean_yield_kg_ha</code> Mean yield (kg/ha), if available <code>reported_production_kg</code> Total production (kg) (optional, used to derive mean yield) <p>If <code>reported_production_kg</code> is provided, the mean yield is computed as:</p> <pre><code>mean_yield_kg_ha = reported_production_kg / cropland_area_ha\n</code></pre> <p>with cropland_area_ha being the computed cropland_area_ha based on the provided cropland map!.</p> <p>Validation data is optional and can also be provided for a subset of years where it is available.  In this case a total reference value is ommited to avoid misinterpretation.</p>"},{"location":"Vercye/inputs/#6-snakemake-configuration","title":"6. Snakemake Configuration","text":"<p>This file defines the study parameters and links the simulation head directory with the pipeline. You will have to adapt this fill for each yield study. An example configuration can be found here example configuration file. We reccomend, to use this as a template for adjustment. The following section describes the meaning of the paramters, but does not represent the syntac for how to organize the config. For this please refer to the example.</p>"},{"location":"Vercye/inputs/#key-parameters","title":"Key Parameters","text":""},{"location":"Vercye/inputs/#general-settings","title":"General Settings","text":"<ul> <li><code>platform</code>: Choose between <code>'local'</code> or <code>'umd'</code> for dependency management.</li> <li><code>sim_study_head_dir</code>: Path to the simulation head directory.</li> <li><code>regions</code>: List of included regions. Must match folder names of regions.</li> <li><code>years</code>: List of included years (<code>int</code>). Must match year folder names.</li> <li><code>timepoints</code>: List of included timepoints. Must match timepoint folder names.</li> <li><code>study_id</code>: Id/Name of that identifies the study. Freely choosable up to a length of 25 characters.</li> <li><code>keep_apsim_db_files</code>: Delete actual APSIM DB files after processing and reporting to free space.(<code>True</code>/<code>False</code>).</li> <li><code>create_per_region_html_report</code>: Create HTML reports of predicted yield curves per region. Requires a lot of space. (<code>True</code>/<code>False</code>).</li> <li><code>title</code>: Free choice of a title, used in the report.</li> <li><code>description</code>: Custom description (freetext) of the study.</li> <li><code>lai_shp_name</code>: Name of the shapefile used for LAI generation. Not used for processing, simply for reference.</li> <li><code>regions_shp_name</code>: Name of the shapefile from which the GeoJSONs where created. Not used for processing, simply for reference.</li> </ul>"},{"location":"Vercye/inputs/#apsim-parameters-apsim_params","title":"APSIM Parameters (<code>apsim_params</code>)","text":"<ul> <li><code>met_source</code>: <code>'NASA_POWER'</code> or <code>'ERA5'</code>. If using <code>ERA5</code> ensure you call <code>earthengine authenticate</code> from your terminal before starting the pipeline.</li> <li><code>met_agg_method</code>\" <code>'centroid'</code> or <code>'mean'</code>. Aggregation method for the pixels within a region. <code>NASA_Power</code> only supports <code>centroid</code>.</li> <li><code>precipitation_source</code>: Choose between <code>'NASA_POWER'</code> or <code>'CHIRPS'</code>. If you are using CHIRPS, you will have to manually download the precipitation data before starting the pipeline (apsim/download_chirps_data.py).</li> <li><code>precipitation_agg_method</code>: Aggregation method for precipitation data (<code>mean</code> or <code>centroid</code>). <code>'NASA_POWER'</code> only supporting <code>centroid</code> currently.</li> <li><code>fallback_precipitation</code>: Set <code>True</code> to use the original precipitation data (<code>NASAPower or ERA5</code>) if <code>CHIRPS</code> is unavailable. CHIRPS only provides coverage from -50 to 50 degrees.</li> <li><code>chirps_dir</code>: Directory containing global CHIRPS precipitation data (tiffs).</li> <li><code>nasapower_cache_dir</code>: Directory (typically outside of your yieldstudy, globally) containing already fetched dates of nasapower met data per region.</li> <li><code>era5_cache_dir</code>: Directory (typically outside of your yieldstudy, globally) containing already fetched dates of nasapower met data per region.</li> <li><code>ee_project</code>: Project ID in google earth engine. Required if using ERA5 meteorological data.</li> <li><code>time_bounds</code>: Defines timepoint parameters:</li> <li><code>sim_start_date</code>, <code>sim_end_date</code>: Start/End date of the simulation in APSIM.</li> <li><code>met_start_date</code>, <code>met_end_date</code>: Start/End date from when to include metereological data into APSIM.</li> </ul>"},{"location":"Vercye/inputs/#lai-parameters-lai_params","title":"LAI Parameters (<code>lai_params</code>)","text":"<ul> <li><code>lai_dir</code>: Directory for LAI data.</li> <li><code>lai_region</code>: Region name with naming convention (<code>{region}_{date}_{resolution}m_LAI.tif</code>).</li> <li><code>lai_resolution</code>: Spatial resolutin, used for matching as above. In meters/pixel.</li> <li><code>crop_name</code>: Specify crop (<code>wheat</code> or <code>maize</code>). Is related to the cropname defined in APSIM.</li> <li><code>use_crop_adjusted_lai</code>: Adjust LAI data for the crop specified (<code>True</code>/<code>False</code>).</li> <li><code>lai_analysis_mode</code>: Set to <code>'raster'</code>.</li> <li><code>time_bounds</code>: LAI start and end dates for each year.</li> <li><code>lai_aggregation</code>: Method how to choose a representative LAI value from a region of pixels. <code>'mean'</code> or <code>'median'</code>.</li> <li><code>smoothed</code>: Whether to smoth the original remotely sensed LAI curve using Sav-Gol Method. (<code>True</code>/<code>False</code>).</li> <li><code>file_ext</code>: LAI files extension. If produced with GEE pipeline and regions are not merged, use <code>tif</code>. Else use <code>vrt</code>. (<code>tif</code>/<code>vrt</code>).</li> <li><code>min_cropland_pixel_threshold</code>: Minimum number of cropland pixels each simulation ROI must contain. If region has less it is skipped. Cropmask pixels are counted at the resolution of the LAI data.</li> <li><code>cloudcov_threshold</code>: Maximum cloud coverage percentage of a region for LAI of a date to be considered valid. E.g if set to 0.9, 90% of the pixels in the LAI data of a specific date must not be clouds/snow, otherwise this date will be ignored in the LAI curve.</li> <li><code>crop_mask</code>: <code>Dict[year, cropmaskpath]</code>. Path to crop mask files for each year. The cropmask must be binary, with 1 indicating the crop and 0 otherwise.</li> <li><code>do_cropmask_reprojection</code>: The cropmask must match the resolution and CRS of the LAI data. If set to <code>True</code> it will be automatically reprojected to match the LAI data. (<code>True</code>/<code>False</code>).</li> </ul>"},{"location":"Vercye/inputs/#matching-parameters-matching_params","title":"Matching Parameters (<code>matching_params</code>)","text":"<ul> <li><code>target_crs</code>: CRS string for coordinate reference system used for area calculation. Either a proj string (e.g <code>'\"+proj=aea +lat_1=29 +lat_2=36 +lat_0=32.5 +lon_0=-5 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\"'</code>) or authority string (e.g <code>'epsg:1234'</code>). Should be choosen with care to minimize distortions. If using a proj string ensure to encolose it with additional quotes as in the example.</li> </ul>"},{"location":"Vercye/inputs/#apsim-execution-apsim_execution","title":"APSIM Execution (<code>apsim_execution</code>)","text":"<ul> <li><code>use_docker</code>: Set <code>True</code> to run APSIM in Docker.</li> <li><code>docker.image</code>: Docker image (<code>apsiminitiative/apsimng</code>).</li> <li><code>docker.platform</code>: Device platform (e.g., <code>'linux/amd64'</code>).</li> <li><code>local.executable_fpath</code>: Path to the local APSIM executable. Follow the setup instruction to install it and replace this path if not using docker.</li> <li><code>local.n_jobs</code>: Number of threads (<code>-1</code> uses APSIM default).</li> </ul>"},{"location":"Vercye/inputs/#evaluation-parameters","title":"Evaluation Parameters","text":"<ul> <li><code>aggregation_levels</code>: Dictionary where the keys are names for levels at which results should be aggregated for evaluation and values are  columns in the shapefile allowing to aggregate simulation level results. E.g a the shapefile might have a column Admin2 and the simulations are run at Admin3. So all predictions that share the same Admin2 region would be aggregated and metrics would be computed for these.</li> </ul>"},{"location":"Vercye/inputs/#scripts-configuration","title":"Scripts Configuration","text":"<ul> <li>Paths to scripts used within the Snakemake pipeline. See the example for details</li> </ul>"},{"location":"Vercye/metdata/","title":"Meteorologcal Data Inputs","text":"<p>TODO description of the different met sources will follow soon.</p>"},{"location":"Vercye/metdata/#nasa-power","title":"NASA Power","text":""},{"location":"Vercye/metdata/#era5","title":"ERA5","text":""},{"location":"Vercye/metdata/#chirps","title":"CHIRPS","text":"<p>Downloading CHIRPS Precipitation Data If you plan on using CHIRPS precipitation data, you will have to download the complete historical and current CHRIPS archive before starting a yield study. For this, we provide a helper script in <code>apsim/download_chirps_data.py</code>. Here you will have to specify the time range (beginning, -end date) and the data will be downloaded with daily global coverage.</p> <pre><code>Usage: download_chirps_data.py [OPTIONS]\n\nOptions:\n  --start-date [%Y-%m-%d]  Start date for CHIRPS data collection in YYYY-MM-DD\n                           format.  [required]\n  --end-date [%Y-%m-%d]    End date for CHIRPS data collection in YYYY-MM-DD\n                           format.  [required]\n  --output-dir TEXT        Output directory to store the CHIRPS data.\n                           [required]\n  --num-workers INTEGER    Number of parallel processes. Capped at 10 due to\n                           server limitations.  [default: 10]\n  --verbose                Enable verbose logging.\n  --help                   Show this message and exit.\n</code></pre> <p>!Attention: This can amount to a few hundred GB of data when downloading many years of historical data. Therefore this is rather intended to be run on HPC environments.</p> <p>This will first try to download all final CHIRPS v2.0 global daily products at 0.05 degrees resolution. For days without data available, the downloader will fallback to the preliminary product.</p> <p>The VeRCYe pipeline will then read local regions from this global files during runtime.</p>"},{"location":"Vercye/outputs/","title":"Outputs","text":""},{"location":"Vercye/outputs/#overview","title":"Overview","text":"<p>The VeRCYe pipeline generates outputs at multiple levels of abstraction, including:</p> <ul> <li>Pixel-wise yield predictions</li> <li>Predictions for each region of interest (ROI)</li> <li>Aggregated insights across multiple ROIs</li> </ul> <p>This document details all output artifacts and their computation methods.</p>"},{"location":"Vercye/outputs/#1-per-region-and-timepoint-outputs","title":"1. Per Region (-and timepoint) Outputs","text":""},{"location":"Vercye/outputs/#reports","title":"Reports","text":"<ul> <li><code>yield_report.html</code>: This summarize the yield study on this region and timepoint. The displayed values are the following:<ul> <li>Number of simulation traces in mean data: The number of APSIM simulations that best match remotely sensed LAI data, after filtering out simulations in <code>match_sim_rs_lai.py</code>. We referr to these as 'matched'</li> <li>Date range: APSIM simulation date range.</li> <li>Mean yield rate: See <code>converted_map_yield_estimate.csv</code> under <code>mean_yield_kg_ha</code>.</li> <li>Production: See <code>converted_map_yield_estimate.csv</code> under <code>total_yield_production_ton</code>.</li> <li>LAI filtered on step x: APSIM Simulated <code>Wheat.Leaf.LAI</code> (or <code>Maize.Leaf.LAI</code>) for a specific simulation and simulation date, with the simulation being filtered out at step x, as defined in <code>match_sim_rs_lai.py</code>.</li> <li>Yield filtered on step x: APSIM Simulated yield for a specific simulation and simulation date, with the simulation being filtered out at step x, as defined in <code>match_sim_rs_lai.py</code>. Yield in kg/ha.</li> <li>RS mean LAI: The remotely sensed mean LAI at each date. The mean is taken over the spatial axes, so the mean of the pixel values of all pixel locations that are within the regions geometry.</li> <li>Mean LAI: The mean simulated LAI eat each date. Hereby the mean for each date, is computed as the mean of all matched (not-filtered out) AP-SIM simulated <code>Wheat.Leaf.LAI</code> (or <code>Maize.Leaf.LAI</code>) values at that date. Yield in</li> <li>Mean Yield: Analogues to <code>Mean LAI</code> for the APSIM simulated yield. All simulated yield values are in kg/ha.</li> </ul> </li> <li><code>yield_report.png</code>: A non-interactive snapshot with lower resolution of the <code>.html</code> yield report.</li> <li><code>weather_report.html</code>: Allows to interactively inspect the meteorological data. All values except the rain graph are computed for the centroid of the region. For the rain graph, if the precipitation data source in the experiment is NASA_Power, this is also for the centroid. If the precipitation data source is CHIRPS, this might either be the value at the centroid or the mean of all CHIRPS values within the region, depending on what was specified in the configuration. </li> </ul>"},{"location":"Vercye/outputs/#maps","title":"Maps","text":"<ul> <li><code>yield_map.tif</code>: Raster output representing the yield in kg/ha per pixel. The pixel values are derived my multiplying the remotely sensed LAI with a conversion factor. The conversion factor is computed from the matched simulations of the region.</li> </ul>"},{"location":"Vercye/outputs/#detailed-outputs","title":"Detailed Outputs","text":"<ul> <li> <p><code>converted_map_yield_estimate.csv</code>: </p> <ul> <li>mean_yield_kg_ha: Mean yield based on pixel level yield predictions from <code>yield_map.tif</code>. Mean over all pixels estimates in kg/ha.</li> <li>median_yield_kg_ha: Median yield based on pixel level yield predictions from <code>yield_map.tif</code>. Medial over all pixel estimates in kg/ha. </li> <li>total_area_ha: Total cropland in region in ha. Computed from number of cropland pixels in region multiplied with the pixel size.</li> <li>total_yield_production_kg: Total yield from the region in kg. Sum over yield per pixel.</li> <li>total_yield_production_ton: <code>total_yield_production_kg</code> / 1000</li> </ul> </li> <li> <p><code>sim_matches.csv</code>: Contains aggregated statistics per APSIM simulation:</p> <ul> <li>SimulationID: The internal ID of the APSIM simulation.</li> <li>Max_Yield: The maximum yield that the APSIM simulation reaches during the specified simulation date range.</li> <li>Max_LAI: The maximum LAI that the APSIM simulation reaches during the specified simulation date range.</li> <li>StepFilteredOut: The step number at which the APSIM simulation was removed from matching candidates, as described in <code>match_sim_rs_lai.py</code>. If this field is empty/nan, the simultion was matched and not filtered out.</li> <li>Further details not publicly available at the moment.</li> </ul> </li> <li> <p><code>conversion_factor.csv</code>: Contains numerous values related to the APSIM simulations (and remotely sensed LAI). <code>Max_Yield</code> and <code>Max_LAI</code> as defined in <code>sim_matches.csv</code>.</p> <ul> <li>apsim_mean_yield_estimate_kg_ha: The mean of <code>Max_Yield</code> over the remaining APSIM simulations fter filtering out simulations as described in <code>match_sim_rs_lai.py</code>.</li> <li>apsim_max_matched_lai: The maximum <code>Max_Sim_LAI</code> value over all matched (filtered) APSIM simulations. </li> <li>apsim_max_matched_lai_date: The correspinding date on which <code>apsim_max_matched_lai</code> is reached.</li> <li>apsim_max_all_lai: The maximum <code>Max_Sim_LAI</code> value over all (not-filtered) APSIM simulations. </li> <li>apsim_max_all_lai_date: The correspinding date on which <code>apsim_max_all_lai</code> is reached.</li> <li>apsim_matched_std_yield_estimate_kg_ha: The standard deviation in kg/ha of <code>Max_Yield</code> over all matched (filtered) APSIM simulations.</li> <li>apsim_all_std_yield_estimate_kg_ha: The standard deviation in kg/ha of <code>Max_Yield</code> over all (not-filtered) APSIM simulations.</li> <li>apsim_matched_maxlai_std: The standard LAI deviation of <code>Max_Sim_LAI</code> over the matched (filtered) APSIM simulations.</li> <li>apsim_all_maxlai_std: The standard LAI deviation of <code>Max_Sim_LAI</code> over all (not-filtered) APSIM simulations.</li> <li>max_rs_lai: For each date the remotely sensed mean LAI is computed (negative values clipped to 0). Hereby the mean is taken spatially over the region. <code>max_rs_lai</code> then defines the maximum LAI value of these. </li> <li>max_rs_lai_date: The correspinding date of the <code>max_rs_lai</code> value.</li> <li>conversion_factor: The factor used to convert the remotely sensed LAI raster to yield estimates that are in kg/ha per pixel.</li> </ul> </li> <li> <p><code>LAI_STATS.csv</code>: Insights on the estimated LAI from the remotely sensed data.     Before computation all negative values are clipped to 0.</p> <ul> <li>Date</li> <li>n_pixels: Number of non-nan pixels in the estimated LAI raster for this date.</li> <li>interpolated: 1 if this LAI value is interpolated based on surrounding values since no remote sensed image was available on this date. 0 it is estimated with the ML model from the remotely sensed image.</li> <li>LAI mean: Mean estimated LAI value over all spatial locations at this date. </li> <li>LAI stddev: Standard deviation of LAI values over all spatial locations at this date.</li> <li>LAI mean adjusted: Mean estimated adjsuted LAI value over all spatial locations at this date. Adjustment is used to adjust for different crops e.g maize or wheat as specified in <code>3_analysis_LAI.py</code>.</li> <li>LAI stddev adjusted: Analogous to <code>LAI stddev</code> for the adjusted LAI.</li> </ul> </li> <li> <p><code>met.csv</code>: Meteorological data fetched for each date in the daterange. If CHIRPS was used for precipitation data, it is NOT included here. Documentation of other columns will be added soon. The Column names follow the NASAPower output format. If ERA5 data is fetched it is converted to the same output format as NASAPower has.</p> </li> <li> <p><code>weather.met</code>: Met file generated from <code>met.csv*</code> that follows the format that APSIM expects. Includes the CHIRPS data in the rain column if CHIPRS is used.</p> </li> <li> <p><code>cropmask_constrained.tif</code>: Binary cropmask (raster) constrained to the region.</p> </li> <li><code>LAI_MAX.tif</code>: Raster of the remotely sensed LAI given as the maximum value per pixel troughout the date range.</li> <li><code>VALID</code> This file simply indicates whether the region meets the minimum cropland pixel requirements and is only used by <code>Snakemake</code>.</li> </ul>"},{"location":"Vercye/outputs/#2-aggregated-outputs","title":"2. Aggregated Outputs","text":"<p>The aggregated outputs are produced for each year-timepoint combination. They combine the artifacts from the indival regions into single files that are easier to work with. The filenames will contain suffixed defined as <code>studyID_year_timepoint</code>, to allow easier sharing of these results.</p> <ul> <li> <p><code>final_report_suffix.pdf</code>: This document gives a final, simple to understand overview of all regional results and preview images of the maps. The description of values not listed below is either documented under the regional output section or the validation outputs section.</p> <ul> <li>date range: Simulation start and end date</li> <li>Met-data Cutoff Date: Date specified in the config file for last data to provide met data to APSIM predictions. Not necessarily the true last date, as data might be unavailable from an earlier timepoint already.</li> <li>total yield: The sum of the estimated yield of all regions.</li> <li>Estimated Yield (Weighted Mean): Sum of predicted production of all simulation level ROIs / sum of total cropland from all simulation level ROIs.</li> <li>Estimated Total Production: Sum of the prediction production from all the simulation level ROIs.</li> <li>Total cropland area: The sum of all the ropland area of all ROIs in ha.</li> <li>Crop productivity pixel level: Visualizes the crop productivity in kg/ha per pixel. Derived from the remotely sensed LAI data coupled with the APSIM simulation trough the conversion factor.</li> </ul> </li> <li> <p><code>agg_yield_estimates_{agglevel}_suffix.csv</code>: The detailed predictions with debug data from all simulation ROIs aggregated by the specified column from the config aggregation level. If <code>primary</code>, it is the simulation level data.</p> </li> <li> <p><code>agg_laicurves_suffix.html</code>: Interactive visualization of the LAI data from all regions in the yield study. Enables quicker debugging.</p> </li> </ul>"},{"location":"Vercye/outputs/#aggregated-maps","title":"Aggregated Maps","text":"<ul> <li><code>yield_map_{aggregation-level}.png</code>: Preview image of all ROI boundaries with the corresponding mean and median yield in kg/ha. The aggregation level defines the name specified in the config by which the simulation level data was aggregated. If <code>primary</code>, it is the simulation level.</li> <li> <p><code>aggregated_yield_map_preview.png</code>: Downsampled image preview of the all regional pixel-level yieldmaps merged into one.</p> </li> <li> <p><code>aggregated_yield_map_suffix.tif</code>: Spatially aggregated yield maps from all regions (See regional <code>yield_map.tif</code>).</p> </li> <li><code>aggregated_LAI_max_suffix.tif</code>: Spatially aggregated LAI_MAX maps from all regions (See regional <code>LAI_MAX.tif</code>).</li> <li><code>aggregated_cropmask_suffix.tif</code>: Spatially aggregated cropmasks from all regions (See regional <code>cropmask_constrained.tif</code>). </li> <li><code>aggregated_region_boundaries_suffix.geojson</code>: All regional geojsons merged into a single one for easier importing into GIS.</li> </ul>"},{"location":"Vercye/outputs/#aggregated-meteorological-insights","title":"Aggregated Meteorological Insights","text":"<ul> <li><code>aggregated_met_stats_suffix.pdf</code>: Averaged meteorological data per region visualized as a vector map. Includes statistics for the specific year and averaged over the last 20 years. Results are computed as per-year averages (grouped by location lat/lon) from the met files.</li> <li><code>aggregated_met_stats_suffix.tif</code>: Same met stats as in the pdf, but as a tif with the same resolution as LAI data. Facilitates analysis for field-scale studies.</li> </ul>"},{"location":"Vercye/outputs/#3-validation-outputs","title":"3. Validation Outputs","text":"<p>If reported data was provided, common metrics are written to the <code>final_report.html</code> and <code>evaluation-{agglevel}.csv</code>. Details regarding the computation are implemented in <code>vercye_ops/matching_sim_real/evaluate_yield_estimates.py</code>.</p>"},{"location":"Vercye/outputs/#error-metrics","title":"Error Metrics","text":"<ul> <li>Mean Error (<code>mean_err_kg_ha</code>): The mean error computed as the mean over the yield errors (reported_mean_yield - predicted_mean_yield) of all study regions. Hereby the predicted and reported yield are the mean yield of the region in kg/ha.</li> <li>Median Error (<code>median_err_kg_ha</code>): The median error computed as the median over the yield errors (reported_mean_yield - predicted_mean_yield) of all study regions. Hereby the predicted and reported yield are the mean yield of the region in kg/ha.</li> <li>Absolute Mean Error (<code>mean_err_kg_ha</code>): Absolute mean error, see above.</li> <li>Absolute Median Error (<code>median_err_kg_ha</code>): Absolute median errorm see above.</li> <li>Root Mean Square Error (<code>rmse</code>): The root mean square error of the predicted and reported mean yields of all study regions in kg/ha.</li> <li>Relative RMSE (<code>rrmse</code>): In percent. Computed as <code>rmse / mean(reported_mean_yield) * 100</code></li> <li>R\u00b2 Score Scikit (<code>r2_scikit</code>): The R2 score of the predicted and reported mean yield per region. Computed as the coefficient of determination with scikit-learn</li> <li>R\u00b2 Score Excel (<code>r2_rsq_excel</code>): The R2 score of the predicted and reported mean yield per region. Computed as the quare of the Pearson product moment correlation coefficient , as implemented in the excel <code>RSQ</code> function See here.</li> </ul>"},{"location":"Vercye/running/","title":"Quickstart - Yield Study","text":"<p>This guide walks you through the process of setting up and running a yield study using our framework, which helps you simulate crop yields across different regions.</p>"},{"location":"Vercye/running/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>Completed the setup and installed all requirements</li> <li>Generated LAI data for your region of interest</li> <li>(Optional) If using CHIRPS precipitation data: Downloaded historical and current CHIRPS precipitation data.</li> <li>(Optional) If using ERA5 Metdata: Run <code>earthengine authenticate</code> and copy the link to your browser to sign into earth engine.</li> </ul>"},{"location":"Vercye/running/#setup-process","title":"Setup Process","text":"<p>Setting up a yield study involves three main steps:</p> <ol> <li>Defining your regions of interest</li> <li>Configuring your simulation parameters</li> <li>Preparing your base directory structure</li> <li>Optional: Adding reference (validation) data</li> </ol> <p>While the individual steps are detailed in other pages of this living document, on this page we outline the quickstart using our setup helper.</p> <p>If you already have prepared a base directory and a configuration file, you can skip to step 4. Otherwise follow the steps below. You will typically want to run this on your local machine and not a remote cluster. You can transfer the final setup to the cluster after creation.</p>"},{"location":"Vercye/running/#1-defining-regions-of-interest","title":"1. Defining Regions of Interest","text":"<p>Your yield study will run simulations for each defined region, typically specified in a shapefile.</p> <p>Important requirements for your shapefile:</p> <ul> <li>Contains geometries at the same administrative level only (e.g., all geometries are districts OR all counties)</li> <li>Includes an attribute column with region names (e.g., <code>NAME_2</code> containing \"Cook County\", \"Orleans Parish\")</li> </ul> <p>[Experimental] If your shapefile contains mixed administrative levels, use the interactive helper script to normalize it to a single level:</p> <pre><code>python apsim/prepare_shapefile.py --shp_fpath /path/to/your.shp --output_dir /path/to/save/dir\n</code></pre>"},{"location":"Vercye/running/#2-defining-your-configuration","title":"2. Defining Your Configuration","text":"<p>Create a configuration file that controls simulation parameters:</p> <ol> <li>Create an empty directory <code>new_basedir_path</code> that will be your basediretory of your study. Recommended to give it a meaningfull name.</li> <li>Navigate to <code>snakemake/example_setup/</code> and copy one of the example configurations to <code>new_basedir_path/config_template.yaml</code>.</li> <li>Modify parameters according to your study needs (years, date ranges, meteorological data sources). For now, leave the regions fields empty, as it will be filled in by the setup helper.</li> </ol> <p>Note The script for matching remotely sensed LAI and APSIM predicted LAI is not publicly available in this repository. You will have to set the path to the true matching script in your <code>config</code> file under <code>scripts.match_sim_real</code>.</p> <p>The full configuration options are documented in the Inputs documentation (Section 6).</p>"},{"location":"Vercye/running/#3-setting-up-your-base-directory","title":"3. Setting Up Your Base Directory","text":"<p>The base directory (your <code>study head dir</code>) organizes region-specific geometries and APSIM simulations by year, timepoint, and region (See Details).</p> <p>Use the provided Jupyter notebook (<code>vercye_setup_helper.ipynb</code>) to create this structure - just set the parameters below in the first cell and run it.</p> <ol> <li> <p>Input shapefile &amp; region names</p> <ul> <li> <p><code>SHAPEFILE_PATH</code>  Path to your <code>.shp</code> containing all simulation-level regions (all geometries must share the same admin level).</p> </li> <li> <p><code>ADMIN_COLUMN_NAME</code>  Attribute column holding each region\u2019s identifier (e.g. <code>NAME_2</code>).</p> </li> </ul> </li> <li> <p>(Optional) Subset regions</p> <p>If you only want a subset of regions (e.g. counties in Texas &amp; Colorado), set:</p> <ul> <li> <p><code>FILTER_COL_NAME</code> Column for the higher-level admin unit (e.g. <code>NAME_1</code>).</p> </li> <li> <p><code>FILTER_COL_VALUES</code> List of values to keep, e.g. <code>['Texas', 'Colorado']</code>.   To include all regions, set <code>FILTER_COL_NAME = None</code> and leave <code>FILTER_COL_VALUES = []</code>.</p> </li> </ul> </li> <li> <p>Intermediate &amp; output folders</p> <ul> <li> <p><code>GEOJSONS_FOLDER</code> Temporary folder where the notebook extracts each region as a GeoJSON polygon.</p> </li> <li> <p><code>OUTPUT_DIR</code> Your new base directory. Place <code>config_template.yaml</code> (your Snakemake config) here.</p> </li> <li> <p><code>SNAKEFILE_CONFIG</code> Path to that prefilled <code>config_template.yaml</code> (it lives in <code>OUTPUT_DIR/config_template.yaml</code>; you can leave its <code>regions:</code> field empty).</p> </li> </ul> </li> <li> <p>APSIM configuration templates</p> <p>Rather than manually copying and editing an APSIM file for each year/region, the helper will:</p> <ol> <li>Copy a template for each higher-level region (e.g. state) into every year\u2019s folder.  </li> <li>Auto-adjust the simulation dates.</li> </ol> <p>Configure this by setting:</p> <ul> <li> <p><code>APSIM_TEMPLATE_PATHS_FILTER_COL_NAME</code> Admin column that groups regions sharing a template (e.g. <code>NAME_1</code>).</p> </li> <li> <p><code>APSIM_TEMPLATE_PATHS</code> Dictionary mapping column values to template paths; e.g. <code>yaml   APSIM_TEMPLATE_PATHS:     Texas:    /path/to/texas_template.yaml     Colorado: /path/to/colorado_template.yaml</code></p> </li> <li> <p>Single-template setup If you only require one APSIM file for all regions, set: <code>yaml   APSIM_TEMPLATE_PATHS_FILTER_COL_NAME: None   APSIM_TEMPLATE_PATHS:     all: /your/path/to/generalApsimTemplate.yaml</code></p> </li> </ul> </li> </ol> <p>Once all parameters are defined, run the notebook. It will:</p> <ul> <li>Create your <code>year/timepoint/region</code> directory tree under <code>OUTPUT_DIR</code>.  </li> <li>Generate a final <code>config.yaml</code> that merges your Snakemake settings with the selected regions.</li> </ul> <p>Note: Sometimes, you might want to add some custom conditionals or processing, that is why we have provided this code in a jupyter notebook. In that case make sure to read the input documentation, to understand the required structure.</p>"},{"location":"Vercye/running/#4-adding-reported-validation-data","title":"4. Adding Reported Validation Data","text":"<p>The VeRCYE pipeline can automatically generate validation metrics (e.g., R\u00b2, RMSE) if reported data is available. To enable this, you must manually add validation data for each year.</p> <p>Validation data can be provided at different geographic scales. It may be available at the smallest unit (e.g., ROI level used in simulations) or at a coarser level (e.g., government statistics). You must specify the scale so VeRCYE can aggregate predictions accordingly.</p> <p>Define aggregation levels in your <code>config file</code> under <code>eval_params.aggregation_levels</code>. For each level, provide a key-value pair where the key is a descriptive name and the value is the column in your original shapefile used for aggregation. For example, if state-level ground truth uses the ADMIN_1 column, specify <code>State: ADMIN_1</code>. If the validation data is at ROI level, no specification is needed\u2014it will be automatically recognized.</p> <p>For each year and aggregation level, create a CSV file named: <code>{year}/groundtruth_{aggregation_name}-{year}.csv</code>, where aggregation_name matches the key in your config (case-sensitive!).</p> <p>Example: For 2024 state-level data, the file should be: <code>basedirectory/2024/groundtruth_State-2024.csv</code> For simulation ROI-level data, use <code>primary</code> as the aggregation name: <code>basedirectory/2024/groundtruth_primary-2024.csv</code></p> <p>CSV Structure</p> <ul> <li><code>region</code>: Name matching GeoJSON folder (for <code>primary aggregation level</code>) or matching attribute table column values for custom aggregation level (Column as specified under <code>eval_params.aggregation_levels</code> in tour <code>config.yaml</code>)</li> <li><code>reported_mean_yield_kg_ha</code>: Mean yield in kg/ha If unavailable, provide <code>reported_production_kg</code> instead. The mean yield will then be calculated using cropmask area (note: subject to cropmask accuracy).If you do not have validation data for certain regions, simply do not include these in your CSV.</li> <li>If your reference data contains area, it is recommended to also include this under <code>reported_area</code> even though this is not yet used in the evaluation pipeline.</li> </ul>"},{"location":"Vercye/running/#5-running-the-yield-study","title":"5. Running the Yield Study","text":"<p>Once your setup is complete:</p> <ol> <li>Transfer your base directory to your HPC cluster (if using one).</li> <li>Adjust the <code>sim_study_head_dir</code> path in <code>config.yaml</code> to match the location you copied the directory to.</li> <li>Navigate to the snakemake directory: <code>cd vercye_ops/vercye_ops/snakemake</code>.</li> <li>Open a <code>tmux</code> session or similar to start the long running job: <code>tmux new -s vercye</code></li> <li>Ensure you have activated your virtual environment if applicable.</li> <li>Run the simulation in the tmux shell (This example will expect to use 110 CPU cores as defined in the <code>profile</code> file.):    <code>bash    snakemake --profile profiles/hpc --configfile /path/to/your/config.yaml</code></li> </ol> <p>For custom CPU core allocation, add the <code>-c</code> flag (e.g with 20 cores) or adapt the <code>profiles/hpc/config.yaml</code> file:    <code>bash    snakemake --profile profiles/hpc --configfile /path/to/your/config.yaml -c 20</code></p>"},{"location":"Vercye/running/#output","title":"Output","text":"<p>When the simulation completes, results will be available in your base directory. See the Outputs Documentation for details on interpreting the results.</p> <p>To run the pipeline over the same region(s), either use Snakemake's <code>-F</code> flag or delete the log files at <code>vercye_ops/snakemake/logs_*</code>. Runtimes are in <code>vercye_ops/snakemake/benchmarks</code>.</p>"}]}