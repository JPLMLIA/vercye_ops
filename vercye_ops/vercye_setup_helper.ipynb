{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "from rasterio.mask import mask as rio_mask\n",
    "\n",
    "from vercye_ops.apsim.convert_shapefile_to_geojson import convert_shapefile_to_geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters and paths\n",
    "# Please refer to the documentation to understand on how to define the config.\n",
    "\n",
    "SHAPEFILE_PATH = '/path/to/shapefile.shp'\n",
    "ADMIN_COLUMN_NAME = 'NAME_2' # Name of the column that contains the administrative division level names for the level of interest in you shapefile.\n",
    "SHAPEFILE_CENTROID_PROJECTION_EPSG = 4326\n",
    "GEOJSONS_FOLDER = \"/path/to/geosons\" # Output folder where to save the extracted geojsons\n",
    "\n",
    "# Optional: Reference raster path - used to get the projection and resolution of the raster for rasterizing the polygons\n",
    "# Typically should be one of the LAI rasters for the region\n",
    "# If none, no check if all regions can be rasterized with at least one pixel will be performed\n",
    "REFERENCE_RASTER_PATH = None\n",
    "\n",
    "# In the beginning you want to start out with a snakemake config file that is completely filled out except for the \"regions\" field\n",
    "# This script will help you fill out the \"regions\" field with the regions extracted from the shapefile\n",
    "SNAKEFILE_CONFIG = \"\" # Config file for the snakemake pipeline\n",
    "OUTPUT_DIR = '' # Directory to save the new head_dir structure and files\n",
    "\n",
    "# TODO allow different apsim templates for different regions/timepoints?\n",
    "APSIM_TEMPLATE_PATH = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts geojsons in with their corresponding vercye-style directories from a shapefile\n",
    "# The geojsons are used as a base for the yieldstudy regions\n",
    "\n",
    "convert_shapefile_to_geojson(shp_fpath=SHAPEFILE_PATH, admin_name_col=ADMIN_COLUMN_NAME, projection_epsg=SHAPEFILE_CENTROID_PROJECTION_EPSG, output_head_dir=GEOJSONS_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the names of the regions that have at least one pixel after rasterization\n",
    "# Copy the names of the regions that you want to keep from the output into your snakemake config under regions\n",
    "\n",
    "for f in os.listdir(GEOJSONS_FOLDER):\n",
    "    geojson_folder_path = os.path.join(GEOJSONS_FOLDER, f)\n",
    "    if not os.path.isdir(geojson_folder_path):\n",
    "        continue\n",
    "\n",
    "    if REFERENCE_RASTER_PATH is None:\n",
    "        print(f\"- '{f}'\")\n",
    "        continue\n",
    "\n",
    "    # Identify number of pixels in rasterized shapefile\n",
    "    shp = gpd.read_file(os.path.join(geojson_folder_path, f + \".geojson\"))\n",
    "    src = rio.open(REFERENCE_RASTER_PATH)\n",
    "    masked_src, masked_transform = rio_mask(src, shp.geometry, crop=False, nodata=0, indexes=1)\n",
    "    num_pixels_rasterized = np.count_nonzero(masked_src)\n",
    "\n",
    "    # Prints region name if at least one pixel is rasterized\n",
    "    if num_pixels_rasterized != 0:\n",
    "        print(f\"- '{f}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates Folder Structure from Config for years/timepoints, copying all regions\n",
    "# Ensure you have filled in your config completely before running this script\n",
    "\n",
    "config = None\n",
    "with open(SNAKEFILE_CONFIG) as snakemake_config_reader:\n",
    "    try:\n",
    "        config = yaml.safe_load(snakemake_config_reader)\n",
    "    except yaml.YAMLError as e:\n",
    "        print(e)\n",
    "\n",
    "years = config['years']\n",
    "timepoints = config['timepoints']\n",
    "regions_names = config['regions']\n",
    "\n",
    "for region_name in regions_names:\n",
    "    region_file_path = os.path.join(GEOJSONS_FOLDER, region_name, f'{region_name}.geojson')\n",
    "    \n",
    "    for year in years:\n",
    "        year_folder = os.path.join(OUTPUT_DIR, str(year))\n",
    "        \n",
    "        for timepoint in timepoints:\n",
    "            timepoint_folder = os.path.join(year_folder, str(timepoint))\n",
    "            \n",
    "            roi_folder = os.path.join(timepoint_folder, region_name)\n",
    "            os.makedirs(roi_folder, exist_ok=True)\n",
    "\n",
    "            shutil.copy(region_file_path, roi_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copies and adjusts APSIM file to each folder. Adjustment applied for start and end dates as defined in snakemake config for the timepoint\n",
    "# Attention!: This script assumes that the APSIM file is the same for all regions and timepoints (except start/end dates)\n",
    "\n",
    "config = None\n",
    "with open(SNAKEFILE_CONFIG) as snakemake_config_reader:\n",
    "    try:\n",
    "        config = yaml.safe_load(snakemake_config_reader)\n",
    "    except yaml.YAMLError as e:\n",
    "        print(e)\n",
    "\n",
    "years = config['years']\n",
    "timepoints = config['timepoints']\n",
    "regions_names = config['regions']\n",
    "\n",
    "for year in years:\n",
    "    year_folder = os.path.join(OUTPUT_DIR, str(year))\n",
    "\n",
    "    for timepoint in timepoints:\n",
    "        timepoint_folder = os.path.join(year_folder, str(timepoint))\n",
    "\n",
    "        for roi in regions_names:  \n",
    "            roi_folder = os.path.join(timepoint_folder, roi)\n",
    "\n",
    "            start_date = config['apsim_params']['time_bounds'][year][timepoint]['sim_start_date']\n",
    "            end_date = config['apsim_params']['time_bounds'][year][timepoint]['sim_end_date']\n",
    "            \n",
    "\n",
    "            with open(APSIM_TEMPLATE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "\n",
    "                # Replace \"Start\" and \"End\" dates with new values\n",
    "                data = re.sub(\n",
    "                    r'\"Start\":\\s*\"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\"', \n",
    "                    f'\"Start\": \"{start_date}T00:00:00\"', \n",
    "                    data\n",
    "                )\n",
    "                data = re.sub(\n",
    "                    r'\"End\":\\s*\"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\"', \n",
    "                    f'\"End\": \"{end_date}T00:00:00\"', \n",
    "                    data\n",
    "                )\n",
    "\n",
    "                # Write new file\n",
    "                print(f'Writing new file for {roi} in {timepoint} of {year} to {roi_folder}')\n",
    "               \n",
    "                new_apsim_path = os.path.join(roi_folder, f'{roi}_template.apsimx')\n",
    "                with open(new_apsim_path, \"w\", encoding=\"utf-8\") as new_file:\n",
    "                    new_file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Place validation data if available in each timepoint (see docs for more info)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
