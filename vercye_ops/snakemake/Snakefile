# type: ignore  # Prevent issues with auto-linting snakefiles
"""This snakefile generates and executes .apsimx simulations given geojson regions and some simulation parameters"""

import os.path as op
from pathlib import Path
from pathvalidate import sanitize_filename
import glob

from snakefile_helpers import build_apsim_execution_command, get_evaluation_results_path_func, get_met_max_range
from generate_aggregated_final_report import create_final_report

def get_valid_regions(wildcards):
    """Get list of valid regions after checkpoint completion"""
    # Force dependency on the checkpoint
    checkpoints.all_regions_validated.get(**wildcards)
    
    base_dir = op.join(config['sim_study_head_dir'], wildcards.year, wildcards.timepoint)
    valid_flags = glob.glob(os.path.join(base_dir, "*", "*_VALID"))
    regions = []
    for vf in valid_flags:
        with open(vf, 'r') as f:
            content = f.read().strip()
        if content == "valid":
            vf_file = Path(vf).stem
            regions.append(vf_file.replace("_VALID", ""))
    return regions

def get_all_valid_region_outputs(wildcards, suffix):
    valid_regions = get_valid_regions(wildcards)
    return [op.join(config['sim_study_head_dir'], wildcards.year, wildcards.timepoint, region, f"{region}_{suffix}") for region in valid_regions]

def get_required_yield_report_suffix():
    """Get the required suffix for the yield report based on the configuration"""

    # Will alwayscreate a lightweight downsampled png, but html report is only created if specifieds
    if config["create_per_region_html_report"]:
        return "html"
    else:
        return "png"

rule all:
    params:
        roi_name = sanitize_filename(config['study_id'])
    input:
        valid_cropmasks = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_VALID'), year=config['years'], region=config['regions'], timepoint=config['timepoints']),
        sim_match_report_workaround = expand(op.join('logs_expand_sim_match_report_region', '{year}_{timepoint}.log'), year=config['years'], timepoint=config['timepoints']),
        # sim_match_output_fpath = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_sim_matches.csv'), year=config['years'], region=get_valid_regions(wildcards), timepoint=config['timepoints']),
        # converted_lai_map = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_yield_map.tif'), year=config['years'], region=get_valid_regions(wildcards), timepoint=config['timepoints']),
        # total_yield_csv = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_converted_map_yield_estimate.csv'), year=config['years'], region=get_valid_regions(wildcards), timepoint=config['timepoints']),
        aggregated_yield_estimates = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'agg_yield_estimates_primary_{}_{}_{}.csv'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')), year=config['years'], timepoint=config['timepoints']),
        aggregated_yield_map = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_yield_map_{}_{}_{}.tif'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')), year=config['years'], timepoint=config['timepoints']),
        aggregated_laicurves = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'agg_laicurves_{}_{}_{}.html'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')), year=config['years'], timepoint=config['timepoints']),
        final_report = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'final_report_{}_{}_{}.pdf'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')), year=config['years'], timepoint=config['timepoints']),
        met_stats = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_met_stats_{}_{}_{}.pdf'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')), year=config['years'], timepoint=config['timepoints']),

# Workaround to expand outputs for individual regions with checkpointing as expand and wildcards don't work well together
rule expand_sim_match_report_region:
    input:
        sim_match_report_html_fpath = lambda wildcards: expand(get_all_valid_region_outputs(wildcards, suffix=f'yield_report.{get_required_yield_report_suffix()}')),
    output:
        'logs_expand_sim_match_report_region/{year}_{timepoint}.log',
    shell:
        """
        touch {output}
        """

#######################################
# APSIM Section of Snakemake pipeline

rule construct_chirps_data:
    priority: 1 # This is a longer running job without dependencies, want to start as early as possible
    input:
        chirps_dir = config['apsim_params']['chirps_dir'],
        geometries = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{{year}}', '{{timepoint}}', '{region}', '{region}.geojson'), region=get_valid_regions(wildcards)),
    params:
        script_fpath = config['scripts']['construct_chirps_data'],
        precipitation_agg_method = config['apsim_params']['precipitation_agg_method'],
        regions_dir = lambda wildcards: op.join(config['sim_study_head_dir'], wildcards.year, wildcards.timepoint), # We choose a fixed one, since all year/timepoints should contain the same regions.
        start_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['met_start_date'],
        end_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['met_end_date'],
    log:
        'logs_construct_chirps_data/{year}_{timepoint}_chirps.log',
    output:
        csv = temp(op.join(config['sim_study_head_dir'], 'temporary_chirps', '{year}_{timepoint}.parquet')),
    shell:
        """
        python {params.script_fpath} \
        --chirps_dir {input.chirps_dir} \
        --regions_base_dir {params.regions_dir} \
        --aggregation_method {params.precipitation_agg_method} \
        --start_date {params.start_date} \
        --end_date {params.end_date} \
        --output_file {output.csv} > {log}
        """

# Before creating the met.csv files with the data per region and timepoint, we fetch the cache with the NASA Power data for the region and all timepoints.
# This needs to be done to avoid multiple calls to the NASA Power API for the same region, which leads to rate limiting.
# This rule fills the cache with the necessary data for the region by taking the min and max date and avoids multiple processes writing to the same cache file.
rule fill_nasapower_cache:
    priority: 1 # This might be a longer running job without dependencies, want to start as early as possible
    input:
        geometry = op.join(config['sim_study_head_dir'], config['years'][0], config['timepoints'][0], '{region}', '{region}.geojson'),
    params:
        met_start_date, met_end_date = get_met_max_range(config)
        script_fpath = config['scripts']['fetch_met_data_nasapower'],
        head_dir = config['sim_study_head_dir'],
        jq_load_statement = 'module load jq' if config['platform'] == 'umd' else '',
        region_name = lambda wildcards: wildcards.region,
        met_agg_method = config['apsim_params']['met_agg_method'],
        np_cache_dir = config['apsim_params']['nasapower_cache_dir'],
    log: 
        'logs_fetch_met_data/{year}_{timepoint}_{region}.log',
    output:
        csv = temp(op.join(config['sim_study_head_dir'], '{region}_np_cache_filled.txt')),
    resources:
        nasa_power_calls = 1
    retries: 2
    shell:
        """
        {params.jq_load_statement}
        read LON LAT <<< $(cat {input}  | jq -r '.features[].properties.centroid' | awk '{{gsub(/POINT \(/, ""); gsub(/\)/, ""); print $1, $2}}')

        python {params.script_fpath} \
        --met_agg_method {params.met_agg_method} \
        --start_date {params.met_start_date} \
        --end_date {params.met_end_date} \
        --variables ALLSKY_SFC_SW_DWN \
        --variables T2M_MAX \
        --variables T2M_MIN \
        --variables T2M \
        --variables PRECTOTCORR \
        --variables WS2M \
        --lon ${{LON}} \
        --lat ${{LAT}} \
        --cache_dir {params.np_cache_dir} \
        --verbose > {log}

        echo "all missing dates fetched" > {output.csv}
        """


# Rule to fetch weather data from NASAPower - at this point the cache should be filled and data is only from the cache and not from the API and save as CSV
rule fetch_met_data_nasapower:
    input:
        cache_fill = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_np_cache_filled.txt'),  # Ensure cache fill is done before fetching data
        geometry = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.geojson'),
    params:
        met_start_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['met_start_date'],
        met_end_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['met_end_date'],
        script_fpath = config['scripts']['fetch_met_data_nasapower'],
        head_dir = config['sim_study_head_dir'],
        jq_load_statement = 'module load jq' if config['platform'] == 'umd' else '',
        region_name = lambda wildcards: wildcards.region,
        met_agg_method = config['apsim_params']['met_agg_method'],
        np_cache_dir = config['apsim_params']['nasapower_cache_dir'],
    log: 
        'logs_fetch_met_data/{year}_{timepoint}_{region}.log',
    output:
        csv = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_met.csv'),
    resources:
        nasa_power_calls = 1
    retries: 2
    shell:
        """
        {params.jq_load_statement}
        read LON LAT <<< $(cat {input}  | jq -r '.features[].properties.centroid' | awk '{{gsub(/POINT \(/, ""); gsub(/\)/, ""); print $1, $2}}')

        python {params.script_fpath} \
        --met_agg_method {params.met_agg_method} \
        --start_date {params.met_start_date} \
        --end_date {params.met_end_date} \
        --variables ALLSKY_SFC_SW_DWN \
        --variables T2M_MAX \
        --variables T2M_MIN \
        --variables T2M \
        --variables PRECTOTCORR \
        --variables WS2M \
        --lon ${{LON}} \
        --lat ${{LAT}} \
        --output_dir {params.head_dir}/{wildcards.year}/{wildcards.timepoint}/{wildcards.region}/ \
        --cache_dir {config['apsim_params']['nasapower_cache_dir']} \
        --verbose > {log}
        """
    ruleorder:
        fetch_met_data_nasapower > fetch_met_data_era5  if config['apsim_params']['met_source'].lower() == 'nasapower' else fetch_met_data_era5 > fetch_met_data_nasapower

# Rule to fetch weather data from ERA5 and save as CSV
# Not yet using caching
rule fetch_met_data_era5:
    input:
        geometry = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.geojson'),
    params:
        met_start_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['met_start_date'],
        met_end_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['met_end_date'],
        script_fpath = config['scripts']['fetch_met_data_era5'],
        head_dir = config['sim_study_head_dir'],
        jq_load_statement = 'module load jq' if config['platform'] == 'umd' else '',
        region_name = lambda wildcards: wildcards.region,
        ee_project = config['apsim_params']['ee_project'],
        met_agg_method = config['apsim_params']['met_agg_method'],
        polygon_path_cmd = lambda wildcards: f"--polygon_path {op.join(config['sim_study_head_dir'], wildcards.year, wildcards.timepoint, wildcards.region, f'{wildcards.region}.geojson')}" if config['apsim_params']['met_agg_method'].lower() == 'mean' else '',
    log: 
        'logs_fetch_met_data/{year}_{timepoint}_{region}.log',
    output:
        csv = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_met.csv'),
    resources:
        era5_calls = 1 
    retries: 5 # ERA5 in GEE can be flaky
    shell:
        """
        {params.jq_load_statement}
        read LON LAT <<< $(cat {input}  | jq -r '.features[].properties.centroid' | awk '{{gsub(/POINT \(/, ""); gsub(/\)/, ""); print $1, $2}}')

        python {params.script_fpath} \
        --met_agg_method {params.met_agg_method} \
        {params.polygon_path_cmd} \
        --start_date {params.met_start_date} \
        --end_date {params.met_end_date} \
        --lon ${{LON}} \
        --lat ${{LAT}} \
        --ee_project {params.ee_project} \
        --output_dir {params.head_dir}/{wildcards.year}/{wildcards.timepoint}/{wildcards.region}/ \
        --overwrite \
        --verbose > {log}
        """
    ruleorder:
        fetch_met_data_era5 > fetch_met_data_nasapower if config['apsim_params']['met_source'].lower() == 'era5' else fetch_met_data_nasapower > fetch_met_data_era5

# Rule to generate .met files from weather CSVs
rule construct_plot_met_files:
    input:
        csv = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_met.csv'),
        geojson = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.geojson'),
    params:
        met_gen_script_fpath = config['scripts']['construct_met_files'],
        met_plot_script_fpath = config['scripts']['plot_met_files'],
        head_dir = config['sim_study_head_dir'],
        sim_end_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['sim_end_date'],
        precipitation_source = config['apsim_params']['precipitation_source'],
        precipitation_agg_method = config['apsim_params']['precipitation_agg_method'],
        fallback_precipitation = config['apsim_params']['fallback_precipitation'],
        jq_load_statement = 'module load jq' if config['platform'] == 'umd' else '',
    log:
        'logs_construct_plot_met_files/{year}_{timepoint}_{region}.log',
    output:
        met_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_weather.met'),
        plot_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_weather_report.html'),
    shell:
        """
        {params.jq_load_statement}
        read LON LAT <<< $(cat {input.geojson}  | jq -r '.features[].properties.centroid' | awk '{{gsub(/POINT \(/, ""); gsub(/\)/, ""); print $1, $2}}')

        python {params.met_gen_script_fpath} \
        --weather_data_fpath {input.csv} \
        --lon ${{LON}} \
        --lat ${{LAT}} \
        --sim_end_date {params.sim_end_date} \
        --precipitation_source {params.precipitation_source} \
        --fallback_precipitation {params.fallback_precipitation} \
        --output_dir {params.head_dir}/{wildcards.year}/{wildcards.timepoint}/{wildcards.region} \
        --verbose > {log}
        
        python {params.met_plot_script_fpath} \
        --input_fpath {output.met_fpath} \
        --output_fpath {output.plot_fpath} \
        --precipitation_source {params.precipitation_source} \
        --precipitation_agg {params.precipitation_agg_method} \
        --fallback_precipitation {params.fallback_precipitation} \
        --met_fpath {input.csv} > {log}
        """
        

# Rule to sub in the .met files to a .apsimx simulation template
rule update_apsimx_template:
    input:
        met_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_weather.met'),
        met_plot_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_weather_report.html'),
    params:
        script_fpath = config['scripts']['update_apsimx_template'],
        head_dir = config['sim_study_head_dir'],
        sim_end_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['sim_end_date']
    log:
        'logs_update_apsimx_template/{year}_{timepoint}_{region}.log',
    output:
        apsimx_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.apsimx')
    shell:
        """
        python {params.script_fpath} \
        --apsimx_template_fpath {params.head_dir}/{wildcards.year}/{wildcards.timepoint}/{wildcards.region}/{wildcards.region}_template.apsimx \
        --apsimx_output_fpath {output.apsimx_fpath} \
        --new_met_fpath {input.met_fpath} \
        --verbose > {log}
        """

# Rule to run the APSIM executable either with docker or local executable on .apsimx files
rule execute_simulations:
    input:
        op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.apsimx'),
    params:
        head_dir = config['sim_study_head_dir'],
        use_docker =  config['apsim_execution']['use_docker'],
        execution_command = lambda wildcards, input: build_apsim_execution_command(
            config['sim_study_head_dir'],
            config['apsim_execution']['use_docker'],
            config['apsim_execution']['docker']['image'],
            config['apsim_execution']['docker']['platform'],
            config['apsim_execution']['local']['executable_fpath'],
            config['apsim_execution']['local']['n_jobs'],
            input)
    log:
        'logs_execute_simulation/{year}_{timepoint}_{region}.log',
    threads: int(config['apsim_execution']['local']['n_jobs'])
    output:
        db_file = (
            op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.db')
            if config['keep_apsim_db_files']
            else temp(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.db'))
        )
    retries: 2 if not config['apsim_execution']['use_docker'] else 5  # Docker image has habit of failing sometimes
    # benchmark:
    #     "benchmarks/execute_simulations_{year}_{timepoint}_{region}.txt"
    shell:
        """
        {params.execution_command} --verbose > {log}
        """

#######################################
# S2/LAI portion of Snakemake pipeline

# Reproject orginal cropmask to match LAI resolution
rule reproject_cropmask:
    input:
        original_cropmask = lambda wildcards: config['lai_params']['crop_mask'][int(wildcards.year)],
    params:
        executable_fpath = config['scripts']['reproject_cropmask'],
        lai_dir = config['lai_params']['lai_dir'],
        lai_region = config['lai_params']['lai_region'],
        lai_resolution = config['lai_params']['lai_resolution'],
        lai_file_ext = config['lai_params']['file_ext'],
    log:
        'logs_reproject_cropmask/{year}.log'
    output:
        reproj_cropmask = op.join(config['sim_study_head_dir'], '{year}', 'reprojected_cropmask.tif')
    shell:
        """
        python {params.executable_fpath} \
        {input.original_cropmask} \
        {output.reproj_cropmask} \
        --lai_dir {params.lai_dir} \
        --lai_region {params.lai_region} \
        --lai_file_ext {params.lai_file_ext} \
        --lai_resolution {params.lai_resolution} > {log}
        """ if config['lai_params']['do_cropmask_reprojection'] else 'cp {input.original_cropmask} {output.reproj_cropmask}'

# Clip the original cropmask
rule constrain_lai_cropmask:
    input:
        original_lai_cropmask = op.join(config['sim_study_head_dir'], '{year}', 'reprojected_cropmask.tif'),  # Will need to update to time period
        geometry_path = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.geojson'),
    params:
        executable_fpath = config['scripts']['constrain_lai_cropmask'],
    log:
        'logs_constrain_lai_cropmask/{year}_{timepoint}_{region}.log'
    output:
        constrained_cropmask = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_cropmask_constrained.tif'),
    shell:
        """
        python {params.executable_fpath} \
        {input.original_lai_cropmask} \
        {input.geometry_path} \
        {output.constrained_cropmask} > {log}
        """

rule validate_region_has_cropland:
    input:
        constrained_cropmask = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_cropmask_constrained.tif')
    output:
        valid_flag_path = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_VALID')
    log:
        'logs_validate_constrained_cropmask/{year}_{timepoint}_{region}.log'
    params:
        executable_fpath = config['scripts']['check_region_has_cropland'],
        min_cropland_px = config['lai_params']['min_cropland_pixel_threshold']
    shell:
        """
        python {params.executable_fpath} \
        --input-path {input.constrained_cropmask} \
        --output-path {output.valid_flag_path} \
        --px-threshold {params.min_cropland_px}
        """

checkpoint all_regions_validated:
    input:
        # This ensures ALL regions have completed validation
        valid_flags = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_VALID'), 
                           year="{year}", timepoint="{timepoint}", region=config['regions'])
    output:
        # Create a simple flag file marking completion of this checkpoint
        touch(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '.all_regions_validated'))

# Run LAI analysis to extract LAI stats and max image
rule lai_analysis:
    input:
        constrained_cropmask = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_cropmask_constrained.tif')
    params:
        executable_fpath = config['scripts']['lai_analysis'],
        lai_dir = config['lai_params']['lai_dir'],
        lai_region = config['lai_params']['lai_region'],
        lai_resolution = config['lai_params']['lai_resolution'],
        mode = config['lai_params']['lai_analysis_mode'],
        adjustment = config['lai_params']['crop_name'] if config['lai_params']['use_crop_adjusted_lai'] else "none",
        start_date = lambda wildcards: config['lai_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint][0],
        end_date = lambda wildcards: config['lai_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint][1],
        lai_file_ext = config['lai_params']['file_ext'],
        smoothed_flag = '--smoothed' if config['lai_params']['smoothed'] is True else '',
        cloudcov_threshold = config['lai_params']['cloudcov_threshold'],
    benchmark:
        "benchmarks/lai_analysis_{year}_{timepoint}_{region}.txt"
    log:
        'logs_lai_analysis/{year}_{timepoint}_{region}.log'
    output:
        stats_csv = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_LAI_STATS.csv'),
        max_lai_tif = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_LAI_MAX.tif'),
    shell:
        """
        python {params.executable_fpath} \
        {params.lai_dir} \
        {output.stats_csv} \
        {output.max_lai_tif} \
        {params.lai_region} \
        {params.lai_resolution} \
        {input.constrained_cropmask} \
        --mode {params.mode} \
        --adjustment {params.adjustment} \
        --start_date {params.start_date} \
        --LAI_file_ext {params.lai_file_ext} \
        --end_date {params.end_date} \
        --cloudcov_threshold {params.cloudcov_threshold} \
        {params.smoothed_flag} > {log}
        """

rule lai_quicklook:
    input:
        stats_csv = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_LAI_STATS.csv'),
        max_lai_tif = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_LAI_MAX.tif'),
    params:
        executable_fpath = config['scripts']['lai_quicklook'],
    log:
        'logs_lai_quicklook/{year}_{timepoint}_{region}.log',
    output:
        output_png = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_LAI_QUICKLOOK.tif'),
    shell:
        """
        python {params.executable_fpath} \
        {input.stats_csv} \
        {input.max_lai_tif} > {log}
        """

#######################################
# Match sim/real (APSIM/S2-LAI) outputs

rule match_sim_real:
    input:
        stats_csv = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_LAI_STATS.csv'),
        db_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.db'),
    params:
        executable_fpath = config['scripts']['match_sim_real'],
        adjustment = "--use_adjusted" if config['lai_params']['use_crop_adjusted_lai'] else "",
        crop_name = config['lai_params']['crop_name'],
        lai_agg_type = config['lai_params']['lai_aggregation'],
    log:
        'logs_match_sim_real/{year}_{timepoint}_{region}.log',
    threads: 10 + 2 # Workaround as i had a lot of issues with the threads in the match_sim_real script
    output:
        sim_matches_output_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_sim_matches.csv'),
        conversion_factor_output_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_conversion_factor.csv'),
    shell:
        """
        python {params.executable_fpath} \
        --rs_lai_csv {input.stats_csv} \
        --db_path {input.db_fpath} \
        --sim_matches_output_fpath {output.sim_matches_output_fpath} \
        --conversion_factor_output_fpath {output.conversion_factor_output_fpath} \
        --crop_name {params.crop_name} \
        --n_jobs 10 \
        {params.adjustment} \
        --lai_agg_type {params.lai_agg_type} \
        --verbose > {log}
        """

rule generate_converted_lai_map:
    input:
        max_lai_tif = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_LAI_MAX.tif'),
        conversion_factor_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_conversion_factor.csv'),
    params:
        executable_fpath = config['scripts']['generate_converted_lai_map'],
        adjustment = "--use_adjusted" if config['lai_params']['use_crop_adjusted_lai'] else "",  # Tells us to use first band (unadjusted) or second band (adjusted)
    log:
        'logs_generate_converted_lai_map/{year}_{timepoint}_{region}.log',
    output:
        converted_lai_map = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_yield_map.tif'),
    shell:
        """
        python {params.executable_fpath} \
        --tif_fpath {input.max_lai_tif} \
        --csv_fpath {input.conversion_factor_fpath} \
        --output_tif_fpath {output.converted_lai_map} \
        {params.adjustment} \
        --verbose > {log}
        """

rule generate_aggregated_lai_curves_plot:
    input:
        stats_csv = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{{year}}', '{{timepoint}}', '{region}', '{region}_LAI_STATS.csv'), region=get_valid_regions(wildcards))
    params:
        executable_fpath = config['scripts']['generate_aggregated_lai_curves_plot'],
        head_dir = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}'),
        lai_agg_type = config['lai_params']['lai_aggregation'],
    output:
        aggregated_lai_curves_plot = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'agg_laicurves_{}_{}_{}.html'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    log:
        'logs_generate_aggregated_lai_curves_plot/{year}_{timepoint}.log',
    shell:
        """
        python {params.executable_fpath} \
        --input-dir {params.head_dir} \
        --output-fpath {output.aggregated_lai_curves_plot} \
        --lai_agg_type {params.lai_agg_type} > {log}
        """


rule estimate_total_yield:
    input:
        converted_lai_map = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_yield_map.tif'),
    params:
        executable_fpath = config['scripts']['estimate_total_yield'],
        target_crs = config['matching_params']['target_crs'],
    log:
        'logs_estimate_total_yield/{year}_{timepoint}_{region}.log',
    output:
        total_yield_csv = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_converted_map_yield_estimate.csv'),
    shell:
        """
        python {params.executable_fpath} \
        --converted_lai_tif_fpath {input.converted_lai_map} \
        --target_crs {params.target_crs} \
        --output_yield_csv_fpath {output.total_yield_csv} \
        --verbose > {log}
        """


rule match_sim_real_quicklook:
    input:
        sim_matches_output_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_sim_matches.csv'),
        rs_stats_csv_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_LAI_STATS.csv'),
        apsim_db_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.db'),
        total_yield_csv_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_converted_map_yield_estimate.csv'),
    params:
        executable_fpath = config['scripts']['match_sim_real_quicklook'],
        crop_name = config['lai_params']['crop_name'],
        adjustment = "--use_adjusted_lai" if config['lai_params']['use_crop_adjusted_lai'] else "",
        lai_agg_type = config['lai_params']['lai_aggregation'],
        html_param = lambda wildcards, output: "--html_fpath " + output.html_fpath if config['create_per_region_html_report'] is True else "",
    log:
        'logs_match_sim_real_quicklook/{year}_{timepoint}_{region}.log',
    output:
        html_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_yield_report.html') if config['create_per_region_html_report'] is True else [],
        png_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_yield_report.png'),
    shell:
        """
        python {params.executable_fpath} \
        --apsim_filtered_fpath {input.sim_matches_output_fpath} \
        --rs_lai_csv_fpath {input.rs_stats_csv_fpath} \
        --apsim_db_fpath {input.apsim_db_fpath} \
        --crop_name {params.crop_name} \
        --total_yield_csv_fpath {input.total_yield_csv_fpath} \
        {params.html_param} \
        {params.adjustment} \
        --lai_agg_type {params.lai_agg_type} \
        --png_fpath {output.png_fpath} > {log}
        """

rule aggregate_yield_estimates:
    input:
        total_yield_csvs = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{{year}}', '{{timepoint}}', '{region}', '{region}_converted_map_yield_estimate.csv'), region=get_valid_regions(wildcards)),
        conversion_factor_fpath = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{{year}}', '{{timepoint}}', '{region}', '{region}_conversion_factor.csv'), region=get_valid_regions(wildcards))
    params:
        executable_fpath = config['scripts']['aggregate_yield_estimates'],
        yield_dir = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}'),
        columns_to_keep_cmd = ' --columns_to_keep ' + ','.join(config['eval_params']['aggregation_levels'].values()) if config['eval_params']['aggregation_levels'] else '',
    log:
        'logs_aggregate_yield_estimates/{year}_{timepoint}.log'
    output:
        aggregated_csv = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'agg_yield_estimates_primary_{}_{}_{}.csv'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    shell:
        """
        python {params.executable_fpath} \
        --yield_dir {params.yield_dir} \
        {params.columns_to_keep_cmd} \
        --output_csv {output.aggregated_csv} \
        --verbose > {log}
        """

rule aggregate_met_stats:
    input:
        met_files = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{{year}}', '{{timepoint}}', '{region}', '{region}_weather.met'), region=get_valid_regions(wildcards)),
        reference_tif_path = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_yield_map_{}_{}_{}.tif'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    params:
        executable_fpath = config['scripts']['aggregate_met_stats'],
        year = lambda wildcards: wildcards.year,
        roi_base_dir = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}')
    log:
        'logs_aggregate_met_stats/{year}_{timepoint}.log'
    output:
        aggregated_met_stats_pdf = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_met_stats_{}_{}_{}.pdf'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
        # aggregated_met_stats_tif = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_met_stats_{}_{}_{}.tif'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    shell:
        """
        python {params.executable_fpath} \
        --regions_base_dir {params.roi_base_dir} \
        --year {params.year} \
        --output_pdf_path {output.aggregated_met_stats_pdf} \
        --verbose > {log}
        """

rule aggregate_maps:
    input:
        cropmasks = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{{year}}', '{{timepoint}}', '{region}', '{region}_cropmask_constrained.tif'), region=get_valid_regions(wildcards)),
        lai_maps = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{{year}}', '{{timepoint}}', '{region}', '{region}_LAI_MAX.tif'), region=get_valid_regions(wildcards)),
        yield_maps = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{{year}}', '{{timepoint}}', '{region}', '{region}_yield_map.tif'), region=get_valid_regions(wildcards)),
        aggregated_yield_estimates = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'agg_yield_estimates_primary_{}_{}_{}.csv'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    params:
        executable_fpath = config['scripts']['aggregate_maps'],
        roi_base_dir = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}'),
        validation_cmd = f"--val_fpath {op.join(config['sim_study_head_dir'], '{year}', 'groundtruth_primary-{year}.csv')}" if op.exists(op.join(config['sim_study_head_dir'], '{year}', 'groundtruth_primary-{year}.csv')) else ''
    log:
        'logs_aggregate_maps/{year}_{timepoint}.log'
    output:
        aggregated_yield_map = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_yield_map_{}_{}_{}.tif'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
        aggregated_lai_map = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_LAI_MAX_{}_{}_{}.tif'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
        aggregated_cropmask = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_cropmask_{}_{}_{}.tif'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
        aggregated_shapefile = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_region_boundaries_{}_{}_{}.geojson'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    shell:
        """
        python {params.executable_fpath} \
        --roi_base_dir {params.roi_base_dir} \
        {params.validation_cmd} \
        --yield_estimates_fpath {input.aggregated_yield_estimates} \
        --output_lai_tif_fpath {output.aggregated_lai_map} \
        --output_yield_tif_fpath {output.aggregated_yield_map} \
        --output_cropmask_tif_fpath {output.aggregated_cropmask} \
        --output_shapefile_fpath {output.aggregated_shapefile} > {log}
        """

rule aggregate_yield_estimates_per_eval_lvl:
    input:
        aggregated_yield_estimates = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'agg_yield_estimates_primary_{}_{}_{}.csv'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    params:
        executable_fpath = config['scripts']['aggregate_yield_estimates_per_eval_lvl'],
        aggregation_col =  lambda wildcards: config['eval_params']['aggregation_levels'][wildcards.agg_level_name],
    log:
        'logs_aggregate_yield_estimates_per_eval_lvl/{year}_{timepoint}_{agg_level_name}.log'
    output:
        lvl_aggregated_yield_estimates = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'agg_yield_estimates_{}_{}_{}_{}.csv'.format(sanitize_filename('{agg_level_name}'), sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    shell:
        """
        python {params.executable_fpath} \
        --estimation_fpath {input.aggregated_yield_estimates} \
        --aggregation_col {params.aggregation_col} \
        --out_fpath {output.lvl_aggregated_yield_estimates} > {log}
        """

rule evaluate_yield_estimates:
    input:
        aggregated_yield_estimates = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'agg_yield_estimates_{}_{}_{}_{}.csv'.format(sanitize_filename('{agg_level_name}'), sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
        validation_fpath = op.join(config['sim_study_head_dir'], '{year}', 'groundtruth_{agg_level_name}-{year}.csv')
    params:
        executable_fpath = config['scripts']['evaluate_yield_estimates'],
        out_dir = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}'),
        out_file_name = lambda wildcards: f'evaluation_{wildcards.agg_level_name}.csv'
    log:
        'logs_evaluate_yield_estimates/{year}_{timepoint}_{agg_level_name}.log'
    output:
        evaluation_out_path = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'evaluation_{agg_level_name}.csv'),
        scatterplot_path = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'evaluation_{agg_level_name}.png'),
    shell:
        """
        python {params.executable_fpath} \
        --val_fpath {input.validation_fpath} \
        --estimation_fpath {input.aggregated_yield_estimates} \
        --out_csv_fpath {output.evaluation_out_path} \
        --out_plot_fpath {output.scatterplot_path} \
        --verbose > {log}
        """

rule generate_final_report:
    input:
        region_geojsons = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.geojson'), 
                                 region=get_valid_regions(wildcards),
                                 year='{year}',
                                 timepoint='{timepoint}'),
        aggregated_yield_estimates = lambda wildcards: expand(
            op.join(config['sim_study_head_dir'], wildcards.year, wildcards.timepoint, 'agg_yield_estimates_{}_{}_{}_{}.csv'
            .format(sanitize_filename('{agg_level_name}'), sanitize_filename(config['study_id']), wildcards.year, wildcards.timepoint)), 
            agg_level_name=list(config['eval_params']['aggregation_levels'].keys()) + ['primary']),
        pixel_level_yieldmap = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 
                                      'aggregated_yield_map_{}_{}_{}.tif'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
        evaluation_results = lambda wildcards: get_evaluation_results_path_func(config)(wildcards)
    params:
        executable_fpath = config['scripts']['generate_final_report'],
        regions_dir = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}'),
        start_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['sim_start_date'],
        end_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['sim_end_date'],
        study_id = sanitize_filename(config['study_id']),
        crop_name = config['lai_params']['crop_name'],
        cutoff_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['met_end_date'],
        met_source = config['apsim_params']['met_source'],
        precipitation_source = config['apsim_params']['precipitation_source'],
        precipitation_agg_method = config['apsim_params']['precipitation_agg_method'],
        fallback_precipitation = config['apsim_params']['fallback_precipitation'],
        primary_suffix = 'primary', # Simulation level
        suffix_admincols = config['eval_params']['aggregation_levels'],
        description = config['description'],
        original_lai_shp = config['lai_shp_name'],
        original_simregions_shp = config['regions_shp_name'],
        title = config['title'],
    log:
        op.join('logs_generate_final_report', '{year}_{timepoint}.log')
    output:
        report_fpath = op.join(config['sim_study_head_dir'], 
                              '{year}',
                              '{timepoint}', 
                              'final_report_{}_{}_{}.pdf'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    run:
        with open(log[0], 'w') as logfile:
            create_final_report(input, output, params, log=logfile, wildcards=wildcards)
