# type: ignore  # Prevent issues with auto-linting snakefiles
"""This snakefile generates and executes .apsimx simulations given geojson regions and some simulation parameters"""

import os.path as op
from pathlib import Path
from pathvalidate import sanitize_filename
import glob

from snakefile_helpers import build_apsim_execution_command, get_evaluation_results_path_func
from generate_aggregated_final_report import create_final_report

def get_valid_regions(wildcards):
    """Get list of valid regions after checkpoint completion"""
    # Force dependency on the checkpoint
    checkpoints.all_regions_validated.get(**wildcards)
    
    base_dir = op.join(config['sim_study_head_dir'], wildcards.year, wildcards.timepoint)
    valid_flags = glob.glob(os.path.join(base_dir, "*", "*_VALID"))
    regions = []
    for vf in valid_flags:
        with open(vf, 'r') as f:
            content = f.read().strip()
        if content == "valid":
            vf_file = Path(vf).stem
            regions.append(vf_file.replace("_VALID", ""))
    return regions

rule all:
    params:
        roi_name = sanitize_filename(config['study_id'])
    input:
        valid_cropmasks = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_VALID'), year=config['years'], region=config['regions'], timepoint=config['timepoints']),
        # sim_match_output_fpath = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_sim_matches.csv'), year=config['years'], region=get_valid_regions(wildcards), timepoint=config['timepoints']),
        # sim_match_report_html_fpath = lambda wildcards: get_sim_match_reports(wildcards),
        #sim_match_report_png_fpath = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_yield_report.png'),  year=config['years'], region=get_valid_regions(wildcards), timepoint=config['timepoints']),
        #converted_lai_map = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_yield_map.tif'), year=config['years'], region=get_valid_regions(wildcards), timepoint=config['timepoints']),
        #sim_match_report_logs = expand('logs_match_sim_real_quicklook/{year}_{timepoint}_{region}.log', year=config['years'], region=get_valid_regions(wildcards), timepoint=config['timepoints']),
        #total_yield_csv = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_converted_map_yield_estimate.csv'), year=config['years'], region=get_valid_regions(wildcards), timepoint=config['timepoints']),
        aggregated_yield_estimates = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'agg_yield_estimates_primary_{}_{}_{}.csv'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')), year=config['years'], timepoint=config['timepoints']),
        aggregated_yield_map = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_yield_map_{}_{}_{}.tif'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')), year=config['years'], timepoint=config['timepoints']),
        aggregated_laicurves = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'agg_laicurves_{}_{}_{}.html'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')), year=config['years'], timepoint=config['timepoints']),
        final_report = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'final_report_{}_{}_{}.pdf'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')), year=config['years'], timepoint=config['timepoints']),
        met_stats = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_met_stats_{}_{}_{}.pdf'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')), year=config['years'], timepoint=config['timepoints']),


#######################################
# APSIM Section of Snakemake pipeline

rule construct_chirps_data:
    priority: 1 # This is a longer running job without dependencies, want to start as early as possible
    input:
        chirps_dir = config['apsim_params']['chirps_dir'],
        geometries = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{{year}}', '{{timepoint}}', '{region}', '{region}.geojson'), region=get_valid_regions(wildcards)),
    params:
        script_fpath = config['scripts']['construct_chirps_data'],
        precipitation_agg_method = config['apsim_params']['precipitation_agg_method'],
        regions_dir = lambda wildcards: op.join(config['sim_study_head_dir'], wildcards.year, wildcards.timepoint), # We choose a fixed one, since all year/timepoints should contain the same regions.
        start_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['met_start_date'],
        end_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['met_end_date'],
    log:
        'logs_construct_chirps_data/{year}_{timepoint}_chirps.log',
    output:
        csv = temp(op.join(config['sim_study_head_dir'], 'temporary_chirps', '{year}_{timepoint}.parquet')),
    shell:
        """
        python {params.script_fpath} \
        --chirps_dir {input.chirps_dir} \
        --regions_base_dir {params.regions_dir} \
        --aggregation_method {params.precipitation_agg_method} \
        --start_date {params.start_date} \
        --end_date {params.end_date} \
        --output_file {output.csv} > {log}
        """

# Rule to fetch weather data from NASAPower or ERA5 and save as CSV
rule fetch_met_data:
    input:
        geometry = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.geojson'),
        chirps_file = op.join(config['sim_study_head_dir'], 'temporary_chirps', '{year}_{timepoint}.parquet') if config['apsim_params']['precipitation_source'].lower() == 'chirps' else []
    params:
        met_source = lambda wildcards: config['apsim_params']['met_source'],
        met_start_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['met_start_date'],
        met_end_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['met_end_date'],
        precipitation_source = config['apsim_params']['precipitation_source'],
        fallback_precipitation = config['apsim_params']['fallback_precipitation'],
        precipitation_agg_method = config['apsim_params']['precipitation_agg_method'],
        chirps_dir = config['apsim_params']['chirps_dir'],
        script_fpath = config['scripts']['fetch_met_data'],
        head_dir = config['sim_study_head_dir'],
        jq_load_statement = 'module load jq' if config['platform'] == 'umd' else '',
        region_name = lambda wildcards: wildcards.region,
        chirps_file = lambda wildcards: f"--chirps_file {op.join(config['sim_study_head_dir'], 'temporary_chirps', f'{wildcards.year}_{wildcards.timepoint}.parquet')}" if config['apsim_params']['precipitation_source'].lower() == 'chirps' else '',
        ee_project = config['apsim_params']['ee_project'],
    log: 
        'logs_fetch_met_data/{year}_{timepoint}_{region}.log',
    output:
        csv = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_met.csv'),
    threads: workflow.cores * 0.75 if config['apsim_params']['met_source'].lower() == 'nasa_power' else workflow.cores * 0.1 # Set this high to limit parallelization and avoid hitting NASA servers too fast and being blacklisted
    retries: 3
    shell:
        """
        {params.jq_load_statement}
        read LON LAT <<< $(cat {input}  | jq -r '.features[].properties.centroid' | awk '{{gsub(/POINT \(/, ""); gsub(/\)/, ""); print $1, $2}}')

        python {params.script_fpath} \
        --met_source {params.met_source} \
        --start_date {params.met_start_date} \
        --end_date {params.met_end_date} \
        --variables ALLSKY_SFC_SW_DWN \
        --variables T2M_MAX \
        --variables T2M_MIN \
        --variables T2M \
        --variables PRECTOTCORR \
        --variables WS2M \
        --lon ${{LON}} \
        --lat ${{LAT}} \
        --precipitation_source {params.precipitation_source} \
        --precipitation_agg_method {params.precipitation_agg_method} \
        --chirps_column_name {params.region_name} \
        --fallback_precipitation {params.fallback_precipitation} \
        {params.chirps_file} \
        --ee_project {params.ee_project} \
        --output_dir {params.head_dir}/{wildcards.year}/{wildcards.timepoint}/{wildcards.region}/ \
        --overwrite \
        --verbose > {log}
        """

# Rule to generate .met files from weather CSVs
rule construct_plot_met_files:
    input:
        csv = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_met.csv'),
        geojson = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.geojson'),
    params:
        met_gen_script_fpath = config['scripts']['construct_met_files'],
        met_plot_script_fpath = config['scripts']['plot_met_files'],
        head_dir = config['sim_study_head_dir'],
        sim_end_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['sim_end_date'],
        precipitation_source = config['apsim_params']['precipitation_source'],
        precipitation_agg_method = config['apsim_params']['precipitation_agg_method'],
        fallback_precipitation = config['apsim_params']['fallback_precipitation'],
        jq_load_statement = 'module load jq' if config['platform'] == 'umd' else '',
    log:
        'logs_construct_plot_met_files/{year}_{timepoint}_{region}.log',
    output:
        met_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_weather.met'),
        plot_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_weather_report.html'),
    shell:
        """
        {params.jq_load_statement}
        read LON LAT <<< $(cat {input.geojson}  | jq -r '.features[].properties.centroid' | awk '{{gsub(/POINT \(/, ""); gsub(/\)/, ""); print $1, $2}}')

        python {params.met_gen_script_fpath} \
        --weather_data_fpath {input.csv} \
        --lon ${{LON}} \
        --lat ${{LAT}} \
        --sim_end_date {params.sim_end_date} \
        --precipitation_source {params.precipitation_source} \
        --fallback_precipitation {params.fallback_precipitation} \
        --output_dir {params.head_dir}/{wildcards.year}/{wildcards.timepoint}/{wildcards.region} \
        --verbose > {log}
        
        python {params.met_plot_script_fpath} \
        --input_fpath {output.met_fpath} \
        --output_fpath {output.plot_fpath} \
        --precipitation_source {params.precipitation_source} \
        --precipitation_agg {params.precipitation_agg_method} \
        --fallback_precipitation {params.fallback_precipitation} \
        --met_fpath {input.csv} > {log}
        """
        

# Rule to sub in the .met files to a .apsimx simulation template
rule update_apsimx_template:
    input:
        met_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_weather.met'),
        met_plot_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_weather_report.html'),
    params:
        script_fpath = config['scripts']['update_apsimx_template'],
        head_dir = config['sim_study_head_dir'],
        sim_end_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['sim_end_date']
    log:
        'logs_update_apsimx_template/{year}_{timepoint}_{region}.log',
    output:
        apsimx_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.apsimx')
    shell:
        """
        python {params.script_fpath} \
        --apsimx_template_fpath {params.head_dir}/{wildcards.year}/{wildcards.timepoint}/{wildcards.region}/{wildcards.region}_template.apsimx \
        --apsimx_output_fpath {output.apsimx_fpath} \
        --new_met_fpath {input.met_fpath} \
        --verbose > {log}
        """

# Rule to run the APSIM executable either with docker or local executable on .apsimx files
rule execute_simulations:
    input:
        op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.apsimx'),
    params:
        head_dir = config['sim_study_head_dir'],
        use_docker =  config['apsim_execution']['use_docker'],
        execution_command = lambda wildcards, input: build_apsim_execution_command(
            config['sim_study_head_dir'],
            config['apsim_execution']['use_docker'],
            config['apsim_execution']['docker']['image'],
            config['apsim_execution']['docker']['platform'],
            config['apsim_execution']['local']['executable_fpath'],
            config['apsim_execution']['local']['n_jobs'],
            input)
    log:
        'logs_execute_simulation/{year}_{timepoint}_{region}.log',
    threads: int(config['apsim_execution']['local']['n_jobs'])
    output:
        db_file = (
            op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.db')
            if config['keep_apsim_db_files']
            else temp(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.db'))
        )
    retries: 2 if not config['apsim_execution']['use_docker'] else 5  # Docker image has habit of failing sometimes
    # benchmark:
    #     "benchmarks/execute_simulations_{year}_{timepoint}_{region}.txt"
    shell:
        """
        {params.execution_command} --verbose > {log}
        """

#######################################
# S2/LAI portion of Snakemake pipeline

# Reproject orginal cropmask to match LAI resolution
rule reproject_cropmask:
    input:
        original_cropmask = lambda wildcards: config['lai_params']['crop_mask'][int(wildcards.year)],
    params:
        executable_fpath = config['scripts']['reproject_cropmask'],
        lai_dir = config['lai_params']['lai_dir'],
        lai_region = config['lai_params']['lai_region'],
        lai_resolution = config['lai_params']['lai_resolution'],
        lai_file_ext = config['lai_params']['file_ext'],
    log:
        'logs_reproject_cropmask/{year}.log'
    output:
        reproj_cropmask = op.join(config['sim_study_head_dir'], '{year}', 'reprojected_cropmask.tif')
    shell:
        """
        python {params.executable_fpath} \
        {input.original_cropmask} \
        {output.reproj_cropmask} \
        --lai_dir {params.lai_dir} \
        --lai_region {params.lai_region} \
        --lai_file_ext {params.lai_file_ext} \
        --lai_resolution {params.lai_resolution} > {log}
        """ if config['lai_params']['do_cropmask_reprojection'] else 'cp {input.original_cropmask} {output.reproj_cropmask}'

# Clip the original cropmask
rule constrain_lai_cropmask:
    input:
        original_lai_cropmask = op.join(config['sim_study_head_dir'], '{year}', 'reprojected_cropmask.tif'),  # Will need to update to time period
        geometry_path = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.geojson'),
    params:
        executable_fpath = config['scripts']['constrain_lai_cropmask'],
    log:
        'logs_constrain_lai_cropmask/{year}_{timepoint}_{region}.log'
    output:
        constrained_cropmask = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_cropmask_constrained.tif'),
    shell:
        """
        python {params.executable_fpath} \
        {input.original_lai_cropmask} \
        {input.geometry_path} \
        {output.constrained_cropmask} > {log}
        """

rule validate_region_has_cropland:
    input:
        constrained_cropmask = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_cropmask_constrained.tif')
    output:
        valid_flag_path = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_VALID')
    log:
        'logs_validate_constrained_cropmask/{year}_{timepoint}_{region}.log'
    params:
        executable_fpath = config['scripts']['check_region_has_cropland']
    shell:
        """
        python {params.executable_fpath} \
        --input-path {input.constrained_cropmask} \
        --output-path {output.valid_flag_path}
        """

checkpoint all_regions_validated:
    input:
        # This ensures ALL regions have completed validation
        valid_flags = expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_VALID'), 
                           year="{year}", timepoint="{timepoint}", region=config['regions'])
    output:
        # Create a simple flag file marking completion of this checkpoint
        touch(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '.all_regions_validated'))

# Run LAI analysis to extract LAI stats and max image
rule lai_analysis:
    input:
        constrained_cropmask = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_cropmask_constrained.tif')
    params:
        executable_fpath = config['scripts']['lai_analysis'],
        lai_dir = config['lai_params']['lai_dir'],
        lai_region = config['lai_params']['lai_region'],
        lai_resolution = config['lai_params']['lai_resolution'],
        mode = config['lai_params']['lai_analysis_mode'],
        adjustment = config['lai_params']['crop_name'] if config['lai_params']['use_crop_adjusted_lai'] else "none",
        start_date = lambda wildcards: config['lai_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint][0],
        end_date = lambda wildcards: config['lai_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint][1],
        lai_file_ext = config['lai_params']['file_ext'],
    benchmark:
        "benchmarks/lai_analysis_{year}_{timepoint}_{region}.txt"
    log:
        'logs_lai_analysis/{year}_{timepoint}_{region}.log'
    output:
        stats_csv = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_LAI_STATS.csv'),
        max_lai_tif = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_LAI_MAX.tif'),
    shell:
        """
        python {params.executable_fpath} \
        {params.lai_dir} \
        {output.stats_csv} \
        {output.max_lai_tif} \
        {params.lai_region} \
        {params.lai_resolution} \
        {input.constrained_cropmask} \
        --mode {params.mode} \
        --adjustment {params.adjustment} \
        --start_date {params.start_date} \
        --LAI_file_ext {params.lai_file_ext} \
        --end_date {params.end_date} > {log}
        """

rule lai_quicklook:
    input:
        stats_csv = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_LAI_STATS.csv'),
        max_lai_tif = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_LAI_MAX.tif'),
    params:
        executable_fpath = config['scripts']['lai_quicklook'],
    log:
        'logs_lai_quicklook/{year}_{timepoint}_{region}.log',
    output:
        output_png = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_LAI_QUICKLOOK.tif'),
    shell:
        """
        python {params.executable_fpath} \
        {input.stats_csv} \
        {input.max_lai_tif} > {log}
        """

#######################################
# Match sim/real (APSIM/S2-LAI) outputs

rule match_sim_real:
    input:
        stats_csv = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_LAI_STATS.csv'),
        db_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.db'),
    params:
        executable_fpath = config['scripts']['match_sim_real'],
        adjustment = "--use_adjusted" if config['lai_params']['use_crop_adjusted_lai'] else "",
        crop_name = config['lai_params']['crop_name'],
    log:
        'logs_match_sim_real/{year}_{timepoint}_{region}.log',
    threads: 10 + 2 # Workaround as i had a lot of issues with the threads in the match_sim_real script
    output:
        sim_matches_output_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_sim_matches.csv'),
        conversion_factor_output_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_conversion_factor.csv'),
    shell:
        """
        python {params.executable_fpath} \
        --rs_lai_csv {input.stats_csv} \
        --db_path {input.db_fpath} \
        --sim_matches_output_fpath {output.sim_matches_output_fpath} \
        --conversion_factor_output_fpath {output.conversion_factor_output_fpath} \
        --crop_name {params.crop_name} \
        --n_jobs 10 \
        {params.adjustment} \
        --verbose > {log}
        """

rule generate_converted_lai_map:
    input:
        max_lai_tif = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_LAI_MAX.tif'),
        conversion_factor_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_conversion_factor.csv'),
    params:
        executable_fpath = config['scripts']['generate_converted_lai_map'],
        adjustment = "--use_adjusted" if config['lai_params']['use_crop_adjusted_lai'] else "",  # Tells us to use first band (unadjusted) or second band (adjusted)
    log:
        'logs_generate_converted_lai_map/{year}_{timepoint}_{region}.log',
    output:
        converted_lai_map = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_yield_map.tif'),
    shell:
        """
        python {params.executable_fpath} \
        --tif_fpath {input.max_lai_tif} \
        --csv_fpath {input.conversion_factor_fpath} \
        --output_tif_fpath {output.converted_lai_map} \
        {params.adjustment} \
        --verbose > {log}
        """

rule generate_aggregated_lai_curves_plot:
    input:
        stats_csv = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{{year}}', '{{timepoint}}', '{region}', '{region}_LAI_STATS.csv'), region=get_valid_regions(wildcards))
    params:
        executable_fpath = config['scripts']['generate_aggregated_lai_curves_plot'],
        head_dir = config['sim_study_head_dir'],
    output:
        aggregated_lai_curves_plot = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'agg_laicurves_{}_{}_{}.html'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    log:
        'logs_generate_aggregated_lai_curves_plot/{year}_{timepoint}.log',
    shell:
        """
        python {params.executable_fpath} \
        --input-dir {params.head_dir} \
        --output-fpath {output.aggregated_lai_curves_plot} > {log}
        """


rule estimate_total_yield:
    input:
        converted_lai_map = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_yield_map.tif'),
    params:
        executable_fpath = config['scripts']['estimate_total_yield'],
        target_crs = config['matching_params']['target_crs'],
    log:
        'logs_estimate_total_yield/{year}_{timepoint}_{region}.log',
    output:
        total_yield_csv = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_converted_map_yield_estimate.csv'),
    shell:
        """
        python {params.executable_fpath} \
        --converted_lai_tif_fpath {input.converted_lai_map} \
        --target_crs {params.target_crs} \
        --output_yield_csv_fpath {output.total_yield_csv} \
        --verbose > {log}
        """


rule match_sim_real_quicklook:
    input:
        sim_matches_output_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_sim_matches.csv'),
        rs_stats_csv_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_LAI_STATS.csv'),
        apsim_db_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.db'),
        total_yield_csv_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_converted_map_yield_estimate.csv'),
    params:
        executable_fpath = config['scripts']['match_sim_real_quicklook'],
        crop_name = config['lai_params']['crop_name'],
        adjustment = "--use_adjusted_lai" if config['lai_params']['use_crop_adjusted_lai'] else "",
    log:
        'logs_match_sim_real_quicklook/{year}_{timepoint}_{region}.log',
    output:
        html_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_yield_report.html'),
        png_fpath = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}_yield_report.png'),
    shell:
        """
        python {params.executable_fpath} \
        --apsim_filtered_fpath {input.sim_matches_output_fpath} \
        --rs_lai_csv_fpath {input.rs_stats_csv_fpath} \
        --apsim_db_fpath {input.apsim_db_fpath} \
        --crop_name {params.crop_name} \
        --total_yield_csv_fpath {input.total_yield_csv_fpath} \
        --html_fpath {output.html_fpath} \
        {params.adjustment} \
        --png_fpath {output.png_fpath} > {log}
        """

rule aggregate_yield_estimates:
    input:
        total_yield_csvs = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{{year}}', '{{timepoint}}', '{region}', '{region}_converted_map_yield_estimate.csv'), region=get_valid_regions(wildcards)),
        conversion_factor_fpath = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{{year}}', '{{timepoint}}', '{region}', '{region}_conversion_factor.csv'), region=get_valid_regions(wildcards))
    params:
        executable_fpath = config['scripts']['aggregate_yield_estimates'],
        yield_dir = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}'),
        columns_to_keep_cmd = ' --columns_to_keep ' + ','.join(config['eval_params']['aggregation_levels'].values()) if config['eval_params']['aggregation_levels'] else '',
    log:
        'logs_aggregate_yield_estimates/{year}_{timepoint}.log'
    output:
        aggregated_csv = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'agg_yield_estimates_primary_{}_{}_{}.csv'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    shell:
        """
        python {params.executable_fpath} \
        --yield_dir {params.yield_dir} \
        {params.columns_to_keep_cmd} \
        --output_csv {output.aggregated_csv} \
        --verbose > {log}
        """

rule aggregate_met_stats:
    input:
        met_files = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{{year}}', '{{timepoint}}', '{region}', '{region}_weather.met'), region=get_valid_regions(wildcards)),
        reference_tif_path = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_yield_map_{}_{}_{}.tif'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    params:
        executable_fpath = config['scripts']['aggregate_met_stats'],
        year = lambda wildcards: wildcards.year,
        roi_base_dir = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}')
    log:
        'logs_aggregate_met_stats/{year}_{timepoint}.log'
    output:
        aggregated_met_stats_pdf = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_met_stats_{}_{}_{}.pdf'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
        aggregated_met_stats_tif = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_met_stats_{}_{}_{}.tif'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    shell:
        """
        python {params.executable_fpath} \
        --regions_base_dir {params.roi_base_dir} \
        --year {params.year} \
        --output_pdf_path {output.aggregated_met_stats_pdf} \
        --reference_tif_path {input.reference_tif_path} \
        --output_tif_path {output.aggregated_met_stats_tif} \
        --verbose > {log}
        """

rule aggregate_maps:
    input:
        cropmasks = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{{year}}', '{{timepoint}}', '{region}', '{region}_cropmask_constrained.tif'), region=get_valid_regions(wildcards)),
        lai_maps = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{{year}}', '{{timepoint}}', '{region}', '{region}_LAI_MAX.tif'), region=get_valid_regions(wildcards)),
        yield_maps = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{{year}}', '{{timepoint}}', '{region}', '{region}_yield_map.tif'), region=get_valid_regions(wildcards)),
        aggregated_yield_estimates = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'agg_yield_estimates_primary_{}_{}_{}.csv'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    params:
        executable_fpath = config['scripts']['aggregate_maps'],
        roi_base_dir = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}'),
        validation_cmd = f"--val_fpath {op.join(config['sim_study_head_dir'], '{year}', 'groundtruth_primary-{year}.csv')}" if op.exists(op.join(config['sim_study_head_dir'], '{year}', 'groundtruth_primary-{year}.csv')) else ''
    log:
        'logs_aggregate_maps/{year}_{timepoint}.log'
    output:
        aggregated_yield_map = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_yield_map_{}_{}_{}.tif'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
        aggregated_lai_map = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_LAI_MAX_{}_{}_{}.tif'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
        aggregated_cropmask = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_cropmask_{}_{}_{}.tif'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
        aggregated_shapefile = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'aggregated_region_boundaries_{}_{}_{}.geojson'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    shell:
        """
        python {params.executable_fpath} \
        --roi_base_dir {params.roi_base_dir} \
        {params.validation_cmd} \
        --yield_estimates_fpath {input.aggregated_yield_estimates} \
        --output_lai_tif_fpath {output.aggregated_lai_map} \
        --output_yield_tif_fpath {output.aggregated_yield_map} \
        --output_cropmask_tif_fpath {output.aggregated_cropmask} \
        --output_shapefile_fpath {output.aggregated_shapefile} > {log}
        """

rule aggregate_yield_estimates_per_eval_lvl:
    input:
        aggregated_yield_estimates = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'agg_yield_estimates_primary_{}_{}_{}.csv'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    params:
        executable_fpath = config['scripts']['aggregate_yield_estimates_per_eval_lvl'],
        aggregation_col =  lambda wildcards: config['eval_params']['aggregation_levels'][wildcards.agg_level_name],
    log:
        'logs_aggregate_yield_estimates_per_eval_lvl/{year}_{timepoint}_{agg_level_name}.log'
    output:
        lvl_aggregated_yield_estimates = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'agg_yield_estimates_{}_{}_{}_{}.csv'.format(sanitize_filename('{agg_level_name}'), sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    shell:
        """
        python {params.executable_fpath} \
        --estimation_fpath {input.aggregated_yield_estimates} \
        --aggregation_col {params.aggregation_col} \
        --out_fpath {output.lvl_aggregated_yield_estimates} > {log}
        """

rule evaluate_yield_estimates:
    input:
        aggregated_yield_estimates = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'agg_yield_estimates_{}_{}_{}_{}.csv'.format(sanitize_filename('{agg_level_name}'), sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
        validation_fpath = op.join(config['sim_study_head_dir'], '{year}', 'groundtruth_{agg_level_name}-{year}.csv')
    params:
        executable_fpath = config['scripts']['evaluate_yield_estimates'],
        out_dir = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}'),
        out_file_name = lambda wildcards: f'evaluation_{wildcards.agg_level_name}.csv'
    log:
        'logs_evaluate_yield_estimates/{year}_{timepoint}_{agg_level_name}.log'
    output:
        evaluation_out_path = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'evaluation_{agg_level_name}.csv'),
        scatterplot_path = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 'evaluation_{agg_level_name}.png'),
    shell:
        """
        python {params.executable_fpath} \
        --val_fpath {input.validation_fpath} \
        --estimation_fpath {input.aggregated_yield_estimates} \
        --out_csv_fpath {output.evaluation_out_path} \
        --out_plot_fpath {output.scatterplot_path} \
        --verbose > {log}
        """

rule generate_final_report:
    input:
        region_geojsons = lambda wildcards: expand(op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', '{region}', '{region}.geojson'), 
                                 region=get_valid_regions(wildcards),
                                 year='{year}',
                                 timepoint='{timepoint}'),
        aggregated_yield_estimates = lambda wildcards: expand(
            op.join(config['sim_study_head_dir'], wildcards.year, wildcards.timepoint, 'agg_yield_estimates_{}_{}_{}_{}.csv'
            .format(sanitize_filename('{agg_level_name}'), sanitize_filename(config['study_id']), wildcards.year, wildcards.timepoint)), 
            agg_level_name=list(config['eval_params']['aggregation_levels'].keys()) + ['primary']),
        pixel_level_yieldmap = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}', 
                                      'aggregated_yield_map_{}_{}_{}.tif'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
        evaluation_results = lambda wildcards: get_evaluation_results_path_func(config)(wildcards)
    params:
        executable_fpath = config['scripts']['generate_final_report'],
        regions_dir = op.join(config['sim_study_head_dir'], '{year}', '{timepoint}'),
        start_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['sim_start_date'],
        end_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['sim_end_date'],
        study_id = sanitize_filename(config['study_id']),
        crop_name = config['lai_params']['crop_name'],
        cutoff_date = lambda wildcards: config['apsim_params']['time_bounds'][int(wildcards.year)][wildcards.timepoint]['met_end_date'],
        met_source = config['apsim_params']['met_source'],
        precipitation_source = config['apsim_params']['precipitation_source'],
        precipitation_agg_method = config['apsim_params']['precipitation_agg_method'],
        fallback_precipitation = config['apsim_params']['fallback_precipitation'],
        primary_suffix = 'primary', # Simulation level
        suffix_admincols = config['eval_params']['aggregation_levels'],
    log:
        op.join('logs_generate_final_report', '{year}_{timepoint}.log')
    output:
        report_fpath = op.join(config['sim_study_head_dir'], 
                              '{year}',
                              '{timepoint}', 
                              'final_report_{}_{}_{}.pdf'.format(sanitize_filename(config['study_id']), '{year}', '{timepoint}')),
    run:
        with open(log[0], 'w') as logfile:
            create_final_report(input, output, params, log=logfile, wildcards=wildcards)
